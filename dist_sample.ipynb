{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11896ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from scipy.stats import gamma, lognorm, norm, skewnorm\n",
    "import random\n",
    "import os\n",
    "import rasterio\n",
    "import glob\n",
    "import datetime as dt\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "def sample_categorical_census(df: pd.DataFrame, category_col: str, value_col: str, ignore_categories: list):\n",
    "    \"\"\"\n",
    "    Sample from a categorical distribution.\n",
    "    \"\"\"\n",
    "\n",
    "    # Filter ignored categories\n",
    "    if ignore_categories:\n",
    "        df = df[~df[category_col].isin(ignore_categories)]\n",
    "\n",
    "    totals = df.groupby(category_col)[value_col].sum()\n",
    "    probs = totals / totals.sum()\n",
    "\n",
    "    samples = np.random.choice(\n",
    "        probs.index.to_numpy(),\n",
    "        size=1,\n",
    "        p=probs.to_numpy()\n",
    "    )\n",
    "\n",
    "    return samples[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a6d969",
   "metadata": {},
   "source": [
    "## Hydrological"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb626db2",
   "metadata": {},
   "source": [
    "### 24-Hour Precipitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67abdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "precipitation = pd.read_csv(\"data/precipitation.csv\")\n",
    "rainfall = precipitation[\"rainfall\"]\n",
    "pct_zero = (rainfall == 0).sum() / len(rainfall)\n",
    "zero_sample = np.random.binomial(n=1, p=pct_zero, size=1)[0]\n",
    "\n",
    "precipitation_nonzero = precipitation[precipitation[\"rainfall\"] != 0].copy()\n",
    "rainfall_nonzero = precipitation[\"rainfall\"]\n",
    "max_prec = max(rainfall_nonzero)\n",
    "shape, loc, scale = gamma.fit(rainfall_nonzero)\n",
    "prec_sample = 0 if zero_sample == 1 else gamma.rvs(a=shape, loc=loc, scale=scale, size=1)[0]\n",
    "print(prec_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fd2f4a",
   "metadata": {},
   "source": [
    "### Groundwater level (WORK IN PREC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ef57c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose random groundwater station, Gamma\n",
    "\n",
    "# mAOD\n",
    "# add prec dependency, modify ground water level based on previous prec_sample\n",
    "\n",
    "groundwater_files = [f for f in os.listdir(\"data/groundwater_level\") if f.lower().endswith(\".csv\")]\n",
    "gw_file = random.choice(groundwater_files)\n",
    "\n",
    "path = os.path.join(\"data/groundwater_level\", gw_file)\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "values = pd.to_numeric(df[\"value\"], errors=\"coerce\").dropna()\n",
    "a, loc, scale = skewnorm.fit(values)\n",
    "gw_sample = skewnorm.rvs(a, loc, scale, size=1)[0]\n",
    "\n",
    "print(f\"Selected file: {gw_file}\")\n",
    "print(gw_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba97ce24",
   "metadata": {},
   "source": [
    "### River flow (WORK IN PREC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a3a8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random choose river station, Gamma\n",
    "\n",
    "# m3/s\n",
    "# add prec dependency, modify ground water level based on previous prec_sample\n",
    "\n",
    "river_flow_files = [f for f in os.listdir(\"data/river_flow\") if f.lower().endswith(\".csv\")]\n",
    "rf_file = random.choice(river_flow_files)\n",
    "\n",
    "path = os.path.join(\"data/river_flow\", rf_file)\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "values = pd.to_numeric(df[\"value\"], errors=\"coerce\").dropna()\n",
    "shape, loc, scale = gamma.fit(values, floc=0)\n",
    "rf_sample = gamma.rvs(a=shape, loc=loc, scale=scale, size=1)[0]\n",
    "\n",
    "print(f\"Selected file: {rf_file}\")\n",
    "print(rf_sample)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f12bb7",
   "metadata": {},
   "source": [
    "### River level (WORK IN PREC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8697fe6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using same river station as river flow, Gamma\n",
    "\n",
    "# m\n",
    "# add prec dependency, modify ground water level based on previous prec_sample\n",
    "\n",
    "file_prefix = rf_file.split('-')[0]\n",
    "river_level_files = [f for f in os.listdir(\"data/river_level\") if f.lower().endswith(\".csv\")]\n",
    "for f in river_level_files:\n",
    "    if file_prefix in f:\n",
    "        rl_file = f\n",
    "        break\n",
    "    \n",
    "path = os.path.join(\"data/river_level\", rl_file)\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "values = pd.to_numeric(df[\"value\"], errors=\"coerce\").dropna()\n",
    "shape, loc, scale = gamma.fit(values, floc=0)\n",
    "rl_sample = gamma.rvs(a=shape, loc=loc, scale=scale, size=1)[0]\n",
    "\n",
    "print(f\"Selected file: {rl_file}\")\n",
    "print(rl_sample)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e530fa6",
   "metadata": {},
   "source": [
    "## Other"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb0199d",
   "metadata": {},
   "source": [
    "### Urban/Rural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec72cb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "urban = pd.read_csv(\"data/urban_rural.csv\")\n",
    "counts = urban['Urban_rural_flag'].value_counts()\n",
    "urban_probs = counts / counts.sum()\n",
    "p_urban = urban_probs['Urban']\n",
    "urban_sample = np.random.binomial(n=1, p=p_urban, size=1)[0]\n",
    "print(urban_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132cb0df",
   "metadata": {},
   "source": [
    "### Population density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6786f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "popden = pd.read_csv(\"data/population_density.csv\", dtype={\"LAD2021\": \"string\", \"OA21CD\": \"string\", \"Total\": \"Int64\"})\n",
    "merged = popden.merge(urban, on=\"OA21CD\", how=\"left\")\n",
    "popden_urbanrural = merged[['LAD2021','OA21CD','Total','Urban_rural_flag']]\n",
    "\n",
    "flag = 'urban' if urban_sample == 1 else 'rural'\n",
    "popden_total  = popden_urbanrural[popden_urbanrural[\"Urban_rural_flag\"].str.lower() == flag][\"Total\"]\n",
    "\n",
    "log = np.log(popden_total)\n",
    "\n",
    "mu, sigma = log.mean(), log.std()\n",
    "\n",
    "popden_sample = np.random.lognormal(mean=mu, sigma=sigma, size=1)[0]\n",
    "print(popden_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8b737c",
   "metadata": {},
   "source": [
    "### Mean property value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4faa335f",
   "metadata": {},
   "outputs": [],
   "source": [
    "property = pd.read_csv(\"data/property_value.csv\")\n",
    "property = property.dropna(subset=['price', 'property_type', 'duration'])\n",
    "property = property[property['price'] > 0]\n",
    "\n",
    "log_property = np.log(property['price'])\n",
    "shape, loc, scale = skewnorm.fit(log_property)\n",
    "sample_log = skewnorm.rvs(shape, loc=loc, scale=scale, size=1)\n",
    "property_sample = np.exp(sample_log)[0]\n",
    "print(property_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a622c9",
   "metadata": {},
   "source": [
    "### Building age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb75b959",
   "metadata": {},
   "outputs": [],
   "source": [
    "building_age = pd.read_csv(\"data/property_age.csv\")\n",
    "age_columns = [\n",
    "    \"BP_PRE_1900\",\"BP_1900_1918\",\"BP_1919_1929\",\"BP_1930_1939\",\n",
    "    \"BP_1945_1954\",\"BP_1955_1964\",\"BP_1965_1972\",\"BP_1973_1982\",\n",
    "    \"BP_1983_1992\",\"BP_1993_1999\",\"BP_2000_2009\",\"BP_2010_2015\"\n",
    "]\n",
    "age_totals = building_age[age_columns].sum()\n",
    "age_totals.index = [\n",
    "    \"Pre-1900\",\"1900-1918\",\"1919-1929\",\"1930-1939\",\"1945-1954\",\"1955-1964\",\n",
    "    \"1965-1972\",\"1973-1982\",\"1983-1992\",\"1993-1999\",\"2000-2009\",\"2010-2015\"\n",
    "]\n",
    "age_probs = age_totals / age_totals.sum()\n",
    "age_categories = age_totals.index.tolist()\n",
    "building_age_sample = np.random.choice(age_categories, size=1, p=age_probs)[0]\n",
    "print(building_age_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace060ae",
   "metadata": {},
   "source": [
    "### Season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febedd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "season = ['Winter', 'Spring', 'Summer', 'Autumn']\n",
    "season_sample = random.choice(season)\n",
    "print(season_sample)\n",
    "\n",
    "SEASON_MONTHS = {\n",
    "    \"Spring\": [3, 4, 5],\n",
    "    \"Summer\": [6, 7, 8],\n",
    "    \"Autumn\": [9, 10, 11],\n",
    "    \"Winter\": [12, 1, 2],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7abfa9",
   "metadata": {},
   "source": [
    "### Holiday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c8d2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Min 28 days off annually\n",
    "'''\n",
    "p = 28 / 365\n",
    "holiday_binary_sample = 1 if random.random() < p else 0\n",
    "print(holiday_binary_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f8daca",
   "metadata": {},
   "source": [
    "### Emergency Response Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd104f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = pd.read_csv(\"data/response_times.csv\")\n",
    "def hhmmss_to_hours(s):\n",
    "    h, m, sec = map(int, s.split(\":\"))\n",
    "    return h + m/60 + sec/3600\n",
    "\n",
    "SEASON_MONTHS_FULL = {\n",
    "    \"Spring\": [\"March\", \"April\", \"May\"],\n",
    "    \"Summer\": [\"June\", \"July\", \"August\"],\n",
    "    \"Autumn\": [\"September\", \"October\", \"November\"],\n",
    "    \"Winter\": [\"December\", \"January\", \"February\"],\n",
    "}\n",
    "months = SEASON_MONTHS_FULL[season_sample]\n",
    "response_season = response[response['month'].isin(months)]\n",
    "\n",
    "c2_mean = response_season[\"C2_mean\"].apply(hhmmss_to_hours)\n",
    "c3_mean = response_season[\"C3_mean\"].apply(hhmmss_to_hours)\n",
    "c2_count = response_season['C2_count'].str.replace(',', '').astype(int).sum()\n",
    "c3_count = response_season['C3_count'].str.replace(',', '').astype(int).sum()\n",
    "c2_prob = c2_count / (c2_count + c3_count)\n",
    "c3_prob = c3_count / (c2_count + c3_count)\n",
    "response_category = np.random.choice(['C2', 'C3'], size=1, p=[c2_prob, c3_prob])[0]\n",
    "if response_category == 'C2':\n",
    "    shape, loc, scale = lognorm.fit(c2_mean, floc=0)\n",
    "else:\n",
    "    shape, loc, scale = lognorm.fit(c3_mean, floc=0)\n",
    "response_sample = lognorm.rvs(shape, loc=loc, scale=scale, size=1)[0]\n",
    "\n",
    "popden_mean = popden_total.mean()\n",
    "if popden_sample < popden_mean: # scale based on average population density\n",
    "    popden_factor = 1 - 0.5 * ((popden_mean - popden_sample) / popden_mean)  # reduce scale\n",
    "else:\n",
    "    popden_factor = 1 + 0.5 * ((popden_sample - popden_mean) / popden_mean)\n",
    "prec_factor = np.exp(prec_sample / 50) # small rain = minimal effect, heavy rain = large effect\n",
    "print(popden_factor, prec_factor)\n",
    "adjusted_response = response_sample * popden_factor * prec_factor\n",
    "print(adjusted_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec6153d",
   "metadata": {},
   "source": [
    "### Ambulance handover delays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a446029",
   "metadata": {},
   "outputs": [],
   "source": [
    "handover = pd.read_csv(\"data/ambulance_handover.csv\")\n",
    "\n",
    "for col in [\"Handover time known\", \"Over 15 minutes\", \"Over 30 minutes\", \"Over 60 minutes\", \"Handover time unknown\", \"All handovers\"]:\n",
    "    handover[col] = handover[col].str.replace(\",\", \"\").astype(int)\n",
    "\n",
    "handover[\"Under 15 min\"] = handover[\"Handover time known\"] - handover[\"Over 15 minutes\"]\n",
    "handover[\"15–30 min\"] = handover[\"Over 15 minutes\"] - handover[\"Over 30 minutes\"]\n",
    "handover[\"30–60 min\"] = handover[\"Over 30 minutes\"] - handover[\"Over 60 minutes\"]\n",
    "handover[\"Over 60 min\"] = handover[\"Over 60 minutes\"]\n",
    "\n",
    "handover[\"Date_parsed\"] = pd.to_datetime(handover[\"Date\"], format=\"%b'%y\")\n",
    "handover_season = handover[handover[\"Date_parsed\"].dt.month.isin(SEASON_MONTHS[season_sample])]\n",
    "\n",
    "counts = handover_season[[\"Under 15 min\", \"15–30 min\", \"30–60 min\", \"Over 60 min\"]].sum()\n",
    "probs = counts / counts.sum()\n",
    "\n",
    "samples = np.random.choice(probs.index.to_numpy(),size=1,p=probs.to_numpy())\n",
    "handover_sample = samples[0]\n",
    "print(handover_sample)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383c6272",
   "metadata": {},
   "source": [
    "### Hospital bed availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4959703e",
   "metadata": {},
   "outputs": [],
   "source": [
    "beds = pd.read_csv(\"data/hospital_beds.csv\")\n",
    "\n",
    "beds[\"Period\"] = pd.to_datetime(beds[\"Period\"], format=\"%d/%m/%Y\")\n",
    "# Aggregate by month (sum across all hospitals)\n",
    "beds = beds.groupby('Period')[['Available', 'Occupied', 'Free']].sum().reset_index()\n",
    "\n",
    "# Compute occupancy percentage\n",
    "beds['OccupancyPct'] = beds['Occupied'] / beds['Available']\n",
    "\n",
    "beds_season = beds[beds[\"Period\"].dt.month.isin(SEASON_MONTHS[season_sample])]\n",
    "values = beds_season[\"OccupancyPct\"].dropna().values\n",
    "\n",
    "mu, sigma = norm.fit(values)\n",
    "occ_pct_sample = np.clip(norm.rvs(mu, sigma), 0, 1)\n",
    "\n",
    "print(occ_pct_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51942825",
   "metadata": {},
   "source": [
    "## Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4626e5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates overall grid shapefile for watercourse, flood risk, elevation, impervious surface area, historic flood, road, hospital locations\n",
    "# recrop area using boundary shapefile for handling updated shapefile/tiff files (GM_shapefile/CAUTH_MAY_2025_EN_BSC.shp)\n",
    "# ignore cell if on or past boundary\n",
    "# cells of size 1km x 1km\n",
    "# centroids used for distance calculations\n",
    "''' \n",
    "data/watercourse/Watercourse.shp\n",
    "- closest watercourse to centre of cell (m)\n",
    "- density of watercourses in cell\n",
    "data/flood_risk/rofsw_4bandPolygon/merged_rofsw_4bandPolygon.shp\n",
    "- confidence-weighted average risk\n",
    "data/elevation.tif\n",
    "- average elevation in cell (m)\n",
    "data/impervious_surface.tif\n",
    "- fraction of impervious surface in cell\n",
    "data/historic_flood_map/Historic_Flood_MapPolygon.shp\n",
    "- if cell has been flooded in the past\n",
    "data/road/RoadLink.shp\n",
    "- closest major road to centre of cell (km)\n",
    "- density of roads in cell\n",
    "data/hospital_locations/hospital_locations.shp\n",
    "- distance to nearest hospital (km)\n",
    "'''\n",
    "\n",
    "'''CELL_SIZE = 1000  # metres\n",
    "CELL_AREA_M2 = CELL_SIZE ** 2\n",
    "CELL_AREA_KM2 = CELL_AREA_M2 / 1e6\n",
    "\n",
    "WATERCOURSE = \"data/watercourse/Watercourse.shp\"\n",
    "FLOOD_RISK = \"data/flood_risk/rofsw_4bandPolygon/merged_rofsw_4bandPolygon.shp\"\n",
    "ELEVATION = \"data/elevation.tif\"\n",
    "IMPERVIOUS = \"data/impervious.tif\"\n",
    "HISTORIC_FLOOD = \"data/historic_flood_map/Historic_Flood_MapPolygon.shp\"\n",
    "ROAD = \"data/road/RoadLink.shp\"\n",
    "HOSPITAL = \"data/hospital_locations/hospital_locations.shp\"\n",
    "BOUNDARY = \"GM_shapefile/CAUTH_MAY_2025_EN_BSC.shp\"\n",
    "\n",
    "OUTPUT = \"data/grid/grid.shp\"\n",
    "\n",
    "RISK_SCORES = {\"Very low\": 1, \"Low\": 2, \"Medium\": 3, \"High\": 4}\n",
    "\n",
    "def build_grid(boundary):\n",
    "    minx, miny, maxx, maxy = boundary.total_bounds\n",
    "\n",
    "    xs = np.arange(minx, maxx, CELL_SIZE)\n",
    "    ys = np.arange(miny, maxy, CELL_SIZE)\n",
    "\n",
    "    grid = gpd.GeoDataFrame(\n",
    "        geometry=[box(x, y, x + CELL_SIZE, y + CELL_SIZE) for x in xs for y in ys],\n",
    "        crs=\"EPSG:27700\"\n",
    "    )\n",
    "\n",
    "    grid = grid[grid.geometry.within(boundary.geometry.union_all())]\n",
    "    grid[\"cell_id\"] = grid.index\n",
    "    return grid\n",
    "\n",
    "def line_density(grid, lines, colname, cell_size):\n",
    "    grid[colname] = 0.0\n",
    "    cell_area_km2 = (cell_size * cell_size) / 1e6  # km²\n",
    "    for i, cell in grid.geometry.items():\n",
    "        inter = lines.geometry.intersection(cell)\n",
    "        length_m = sum(geom.length for geom in inter if not geom.is_empty)\n",
    "        grid.at[i, colname] = (length_m / 1000) / cell_area_km2\n",
    "    return grid\n",
    "\n",
    "\n",
    "def nearest_distance(grid, targets, colname, km=True):\n",
    "    centroids = grid.copy()\n",
    "    centroids[\"geometry\"] = centroids.geometry.centroid\n",
    "\n",
    "    nearest = gpd.sjoin_nearest(\n",
    "        centroids,\n",
    "        targets[[\"geometry\"]],\n",
    "        how=\"left\",\n",
    "        distance_col=colname\n",
    "    )\n",
    "\n",
    "    dist = nearest.groupby(nearest.index)[colname].first()\n",
    "    if km:\n",
    "        dist = dist / 1000\n",
    "\n",
    "    grid[colname] = dist\n",
    "    return grid\n",
    "\n",
    "\n",
    "def flood_risk_score(grid, risk):\n",
    "    inter = gpd.overlay(\n",
    "        grid[[\"cell_id\", \"geometry\"]],\n",
    "        risk,\n",
    "        how=\"intersection\"\n",
    "    )\n",
    "\n",
    "    if inter.empty:\n",
    "        grid[\"risk_score\"] = 0.0\n",
    "        return grid\n",
    "\n",
    "    inter[\"area\"] = inter.geometry.area\n",
    "    inter[\"risk_value\"] = inter[\"risk_band\"].map(RISK_SCORES).fillna(0)\n",
    "    inter[\"conf_weight\"] = inter[\"confidence\"] / 10\n",
    "\n",
    "    inter[\"num\"] = inter[\"area\"] * inter[\"risk_value\"] * inter[\"conf_weight\"]\n",
    "    inter[\"den\"] = inter[\"area\"] * inter[\"conf_weight\"]\n",
    "\n",
    "    agg = inter.groupby(\"cell_id\")[[\"num\", \"den\"]].sum()\n",
    "    grid[\"risk_score\"] = (agg[\"num\"] / agg[\"den\"]).reindex(grid.cell_id).fillna(0)\n",
    "    return grid\n",
    "\n",
    "\n",
    "def zonal_mean(grid, raster_path, colname):\n",
    "    stats = zonal_stats(\n",
    "        grid.geometry,\n",
    "        raster_path,\n",
    "        stats=\"mean\",\n",
    "        nodata=0\n",
    "    )\n",
    "    grid[colname] = [s[\"mean\"] for s in stats]\n",
    "    return grid\n",
    "\n",
    "\n",
    "def zonal_fraction_nonzero(grid, raster_path, colname):\n",
    "    stats = zonal_stats(\n",
    "        grid.geometry,\n",
    "        raster_path,\n",
    "        stats=[\"count\", \"nodata\"],\n",
    "        add_stats={\"nonzero\": lambda x: np.count_nonzero(x)}\n",
    "    )\n",
    "    grid[colname] = [\n",
    "        s[\"nonzero\"] / s[\"count\"] if s[\"count\"] else 0\n",
    "        for s in stats\n",
    "    ]\n",
    "    return grid\n",
    "\n",
    "\n",
    "def historic_flood_flag(grid, historic):\n",
    "    inter = gpd.overlay(\n",
    "        grid[[\"cell_id\", \"geometry\"]],\n",
    "        historic,\n",
    "        how=\"intersection\"\n",
    "    )\n",
    "    inter[\"area\"] = inter.geometry.area\n",
    "    flooded = inter.groupby(\"cell_id\")[\"area\"].sum()\n",
    "\n",
    "    grid[\"historic\"] = (\n",
    "        flooded / CELL_AREA_M2 > 0.5\n",
    "    ).reindex(grid.cell_id).fillna(False).astype(int)\n",
    "\n",
    "    return grid\n",
    "\n",
    "\n",
    "# LOAD DATA\n",
    "print(\"Loading data...\")\n",
    "\n",
    "water = gpd.read_file(WATERCOURSE).set_crs(epsg=27700, allow_override=True)\n",
    "risk = gpd.read_file(FLOOD_RISK).set_crs(epsg=27700, allow_override=True)\n",
    "historic = gpd.read_file(HISTORIC_FLOOD).set_crs(epsg=27700, allow_override=True)\n",
    "road = gpd.read_file(ROAD).set_crs(epsg=27700, allow_override=True)\n",
    "hospital = gpd.read_file(HOSPITAL).set_crs(epsg=27700, allow_override=True)\n",
    "boundary = gpd.read_file(BOUNDARY).set_crs(epsg=27700, allow_override=True)\n",
    "\n",
    "# GRID\n",
    "print(\"Building grid...\")\n",
    "grid = build_grid(boundary)\n",
    "grid.to_file(\"data/grid/grid_step_01.shp\")\n",
    "\n",
    "# LINE DENSITIES\n",
    "print(\"Calculating watercourse density...\")\n",
    "grid = line_density(grid, water, \"water_dens\", CELL_SIZE)\n",
    "\n",
    "print(\"Calculating road density...\")\n",
    "grid = line_density(grid, road, \"road_dens\", CELL_SIZE)\n",
    "\n",
    "# NEAREST DISTANCES\n",
    "print(\"Calculating nearest watercourse distance...\")\n",
    "grid = nearest_distance(grid, water, \"water_dist\", km=False)\n",
    "\n",
    "print(\"Calculating nearest hospital distance...\")\n",
    "grid = nearest_distance(grid, hospital, \"hospital\")\n",
    "\n",
    "print(\"Calculating nearest major road distance...\")\n",
    "major_roads = road[road[\"function\"].isin([\"A Road\", \"Motorway\"])]\n",
    "grid = nearest_distance(grid, major_roads, \"road_dist\")\n",
    "\n",
    "# FLOOD RISK\n",
    "print(\"Calculating flood risk score...\")\n",
    "grid = flood_risk_score(grid, risk)\n",
    "\n",
    "# RASTER FEATURES\n",
    "print(\"Calculating elevation...\")\n",
    "grid = zonal_mean(grid, ELEVATION, \"elevation\")\n",
    "\n",
    "print(\"Calculating impervious fraction...\")\n",
    "grid = zonal_fraction_nonzero(grid, IMPERVIOUS, \"impervious\")\n",
    "\n",
    "# HISTORIC FLOOD\n",
    "print(\"Calculating historic flood flag...\")\n",
    "grid = historic_flood_flag(grid, historic)\n",
    "\n",
    "# SAVE OUTPUT\n",
    "cols = [\n",
    "    \"water_dens\", \"water_dist\", \"risk_score\", \"elevation\",\n",
    "    \"impervious\", \"historic\", \"road_dens\", \"road_dist\", \"hospital\", \"geometry\"\n",
    "]\n",
    "\n",
    "grid[cols].to_file(OUTPUT)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8da0865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from overall grid shapefile\n",
    "gdf = gpd.read_file('data/grid/grid.shp')\n",
    "features = [\n",
    "    \"water_dens\", \"water_dist\", \"risk_score\", \"elevation\",\n",
    "    \"impervious\", \"historic\", \"road_dens\", \"road_dist\", \"hospital\"\n",
    "]\n",
    "\n",
    "def sample_cell_with_noise(gdf, features, noise_scale=0.05):\n",
    "    row = gdf.sample(1).iloc[0]\n",
    "    sample = {}\n",
    "    for f in features:\n",
    "        val = row[f]\n",
    "        noise = np.random.normal(0, noise_scale * abs(val + 1e-6))\n",
    "        sample[f] = max(val + noise, 0)\n",
    "    sample[\"geometry\"] = row.geometry \n",
    "    return sample\n",
    "\n",
    "def sample_neighborhood(gdf, features, k=5):\n",
    "    cell = gdf.sample(1)\n",
    "    dists = gdf.geometry.distance(cell.geometry.iloc[0])\n",
    "    neighbors = gdf.loc[dists.nsmallest(k).index]\n",
    "    return neighbors[features].mean().to_dict()\n",
    "\n",
    "grid_sample = sample_cell_with_noise(gdf, features)\n",
    "neighborhood_sample = sample_neighborhood(gdf, features)\n",
    "print(grid_sample)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1567f536",
   "metadata": {},
   "source": [
    "### Soil moisture saturation (WORK IN PREC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868df05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose pixel that contains the overall grid cell chosen\n",
    "# sample from normal distribution of time-series values for the pixel\n",
    "# filters by season\n",
    "\n",
    "cell_geom = grid_sample[\"geometry\"]  # polygon of the sampled grid cell\n",
    "\n",
    "# Load soil moisture rasters\n",
    "tiff_files = sorted(glob.glob(\"data/soil_moisture/*.tif\"))\n",
    "season_tiffs = []\n",
    "\n",
    "for f in tiff_files:\n",
    "    # Extract date from filename: dt_smuk_2023-12-22.tif\n",
    "    date_str = f.split(\"_\")[-1].replace(\".tif\", \"\")\n",
    "    file_date = dt.datetime.strptime(date_str, \"%Y-%m-%d\")\n",
    "    if file_date.month in SEASON_MONTHS[season_sample]:\n",
    "        season_tiffs.append(f)\n",
    "\n",
    "# Load soil moisture rasters\n",
    "stack = []\n",
    "with rasterio.open(season_tiffs[0]) as src:\n",
    "    transform = src.transform\n",
    "    nodata = src.nodata\n",
    "    for f in season_tiffs:\n",
    "        with rasterio.open(f) as s:\n",
    "            data = s.read(1).astype(np.float32)\n",
    "            if nodata is not None:\n",
    "                data[data == nodata] = np.nan\n",
    "            stack.append(data)\n",
    "\n",
    "stack = np.stack(stack, axis=0)  # shape: (time, rows, cols)\n",
    "\n",
    "# Find the single pixel containing the centroid of the sampled grid cell\n",
    "centroid_x, centroid_y = cell_geom.centroid.x, cell_geom.centroid.y\n",
    "col, row = ~transform * (centroid_x, centroid_y)\n",
    "row, col = int(row), int(col)\n",
    "\n",
    "# Ensure row/col are within raster bounds\n",
    "row = np.clip(row, 0, stack.shape[1]-1)\n",
    "col = np.clip(col, 0, stack.shape[2]-1)\n",
    "\n",
    "# Extract time series for that pixel\n",
    "values = stack[:, row, col]\n",
    "values = values[~np.isnan(values)]\n",
    "\n",
    "# Fit normal distribution safely\n",
    "if len(values) == 0:\n",
    "    soil_sample = np.nan\n",
    "else:\n",
    "    mu, sigma = norm.fit(values)\n",
    "    sigma = max(sigma, 1e-6)\n",
    "    soil_sample = norm.rvs(mu, sigma)\n",
    "\n",
    "print(soil_sample)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96885526",
   "metadata": {},
   "source": [
    "### Flood depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c785ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "to get the probability of exceeding each depth threshold\n",
    "- each shapefile shows area at or above a certain flood depth\n",
    "- e.g. the clipped cell of each shapefile contains multiple polygons overlapping\n",
    "- using rofsw_4bandPolygon.shp get the confidence (0-1), take into account the risk_band (Very low, Low, Medium, High), using the area of the cell that is flooded at each threshold\n",
    "- calculate the probability of flood depth less than each range\n",
    "- e.g. P(depth > 0.2) = area_flooded_at_0.2 / total_area\n",
    "- P(depth <= 0.2) = 1 - P(depth > 0.2)\n",
    "- e.g. P(depth > 0.3) = area_flooded_at_0.3 / total_area\n",
    "- P(depth <= 0.3 and > 0.2) = P(depth <= 0.3) - P(depth <= 0.2)\n",
    "- based on those sample flood depth range probabilistically (0, 0-0.2, 0.2-0.3, 0.3-0.6, 0.6-0.9, 0.9-1.2, >1.2)\n",
    "'''\n",
    "clipped_data = {}\n",
    "\n",
    "for shp_path in glob.glob('data/flood_risk/*/*.shp'):\n",
    "    gdf_shp = gpd.read_file(shp_path)\n",
    "    gdf_shp_clipped = gdf_shp.clip(grid_sample[\"geometry\"].bounds)\n",
    "    \n",
    "    # Store clipped data\n",
    "    key = os.path.basename(shp_path).replace(\".shp\",\"\")\n",
    "    clipped_data[key] = gdf_shp_clipped\n",
    "\n",
    "print(\"Clipped shapefiles:\", list(clipped_data.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754b3d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration: depth thresholds and risk-band weights\n",
    "DEPTH_THRESHOLDS = [0.2, 0.3, 0.6, 0.9, 1.2]\n",
    "\n",
    "RISK_BAND_WEIGHTS = {\n",
    "    \"Very Low\": 0.25,\n",
    "    \"Low\": 0.5,\n",
    "    \"Medium\": 0.75,\n",
    "    \"High\": 1.0\n",
    "}\n",
    "\n",
    "# Load and clip RoFSW polygons\n",
    "rofsw = gpd.read_file(\n",
    "    \"data/flood_risk/rofsw_4bandPolygon/merged_rofsw_4bandPolygon.shp\"\n",
    ")\n",
    "rofsw = rofsw.clip(gpd.GeoSeries([grid_sample[\"geometry\"]]))\n",
    "\n",
    "\n",
    "# Load and clip depth-threshold shapefiles\n",
    "threshold_layers = {}\n",
    "\n",
    "for shp_path in glob.glob(\"data/flood_risk/*/*.shp\"):\n",
    "    if \"rofsw_4bandPolygon\" in shp_path:\n",
    "        continue\n",
    "\n",
    "    gdf = gpd.read_file(shp_path)\n",
    "    gdf = gdf.clip(gpd.GeoSeries([grid_sample[\"geometry\"]]))\n",
    "\n",
    "    if not gdf.empty:\n",
    "        key = os.path.basename(shp_path)\n",
    "        threshold_layers[key] = gdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1720f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare weights and grid-cell area\n",
    "cell_area = grid_sample[\"geometry\"].area\n",
    "\n",
    "rofsw[\"risk_weight\"] = rofsw[\"risk_band\"].map(RISK_BAND_WEIGHTS)\n",
    "rofsw[\"combined_weight\"] = rofsw[\"confidence\"] * rofsw[\"risk_weight\"]\n",
    "\n",
    "# Calculate P(depth > 0) based on RoFSW\n",
    "if rofsw.empty:\n",
    "    p_flood = 0.0\n",
    "else:\n",
    "    # Fraction of cell that is flooded (any depth > 0)\n",
    "    rofsw_weighted_area = (rofsw.geometry.area * rofsw[\"combined_weight\"]).sum()\n",
    "    p_flood = min(rofsw_weighted_area / cell_area, 1.0)\n",
    "\n",
    "# Function to compute weighted exceedance probability\n",
    "def weighted_flood_fraction(threshold_gdf):\n",
    "    if threshold_gdf.empty:\n",
    "        return 0.0\n",
    "\n",
    "    overlay = gpd.overlay(threshold_gdf, rofsw, how=\"intersection\")\n",
    "    if overlay.empty:\n",
    "        return 0.0\n",
    "\n",
    "    overlay[\"weighted_area\"] = overlay.geometry.area * overlay[\"combined_weight\"]\n",
    "    return overlay[\"weighted_area\"].sum() / cell_area\n",
    "\n",
    "# Compute P(depth > d) for each threshold\n",
    "exceedance_probs = {0: p_flood}\n",
    "for depth in DEPTH_THRESHOLDS:\n",
    "    matching = [gdf for name, gdf in threshold_layers.items() if f\"{depth}\".replace(\".\", \"\") in name.replace(\"_\", \"\")]\n",
    "    exceedance_probs[depth] = weighted_flood_fraction(matching[0])\n",
    "\n",
    "# Calculate probabilities for each depth range\n",
    "breaks = [0, 0.2, 0.3, 0.6, 0.9, 1.2]\n",
    "probs = {(0.0, 0.0): 1 - exceedance_probs[0], (1.2, np.inf): exceedance_probs[1.2]}\n",
    "for i in range(len(breaks)-1):\n",
    "    a, b = breaks[i], breaks[i+1]\n",
    "    probs[(a, b)] = exceedance_probs[a] - exceedance_probs[b]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae995939",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(probs)\n",
    "# Sample a range\n",
    "bins = list(probs.keys())\n",
    "range_probs = np.array(list(probs.values()))\n",
    "range_index = np.random.choice(len(bins), p=range_probs)\n",
    "low, high = bins[range_index]\n",
    "\n",
    "# Sample within the selected range\n",
    "if low == 0.0 and high == 0.0:\n",
    "    sampled_depth = 0.0\n",
    "elif np.isinf(high):\n",
    "    sampled_depth = low + np.random.exponential(scale=0.3)\n",
    "else:\n",
    "    sampled_depth = np.random.uniform(low, high)\n",
    "\n",
    "print(\"\\nSelected depth range:\", (low, high))\n",
    "print(\"Sampled flood depth (m):\", round(sampled_depth, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4fcf58",
   "metadata": {},
   "source": [
    "### Depth-damage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118102ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/depth_damage.csv\")\n",
    "depths = np.array([float(c) for c in df.columns[1:]])\n",
    "\n",
    "# Compute overall damage fraction (mean across all types)\n",
    "overall_damage = df.iloc[:, 1:].astype(float).mean(axis=0).values\n",
    "\n",
    "# Define exponential damage function\n",
    "def exp_damage(d, k):\n",
    "    return 1 - np.exp(-k * d)\n",
    "\n",
    "# Fit the exponential model\n",
    "params, _ = curve_fit(exp_damage, depths, overall_damage, bounds=(0, np.inf))\n",
    "k = params[0]\n",
    "\n",
    "# Add noise and ensures stays within bounds\n",
    "damage_fraction_sample = np.clip(exp_damage(sampled_depth, k) + np.random.normal(0, 0.05), 0, 1)\n",
    "print(damage_fraction_sample)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0363a0",
   "metadata": {},
   "source": [
    "## Household"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a95a9cd",
   "metadata": {},
   "source": [
    "### Disability rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f779578",
   "metadata": {},
   "outputs": [],
   "source": [
    "disability = pd.read_csv(\"data/disabled.csv\")\n",
    "total_disability = disability.groupby('Disability (3 categories)')['Observation'].sum()\n",
    "\n",
    "alpha = total_disability['Disabled under the Equality Act'] + 1\n",
    "beta = total_disability['Not disabled under the Equality Act'] + 1\n",
    "\n",
    "disabled_sample = np.random.beta(alpha, beta, size=1)[0]\n",
    "print(disabled_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a36ab1",
   "metadata": {},
   "source": [
    "### English proficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db365f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_proficiency(category):\n",
    "    if category in [\n",
    "        \"Main language is English (English or Welsh in Wales)\",\n",
    "        \"Main language is not English (English or Welsh in Wales): Can speak English very well or well\"\n",
    "    ]:\n",
    "        return \"Good English Proficiency\"\n",
    "    elif category == \"Main language is not English (English or Welsh in Wales): Cannot speak English or cannot speak English well\":\n",
    "        return \"Bad English Proficiency\"\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "english = pd.read_csv(\"data/english_proficiency.csv\")   \n",
    "total_english = english.groupby('Proficiency in English language (4 categories)')['Observation'].sum()\n",
    "english['Proficiency_Group'] = english['Proficiency in English language (4 categories)'].apply(map_proficiency)\n",
    "grouped_english = english.groupby('Proficiency_Group')['Observation'].sum()\n",
    "#english_probs = grouped_english / grouped_english.sum()\n",
    "\n",
    "alpha = grouped_english['Good English Proficiency'] + 1\n",
    "beta = grouped_english['Bad English Proficiency'] + 1\n",
    "\n",
    "english_sample = np.random.beta(alpha, beta, size=1)[0]\n",
    "print(english_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c43566",
   "metadata": {},
   "source": [
    "### General health"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fc474f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/general_health.csv')\n",
    "gen_health_sample = sample_categorical_census(df, \n",
    "                                                'General health (4 categories)', \n",
    "                                                'Observation', \n",
    "                                                ['Does not apply'])\n",
    "print(gen_health_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30e2b24",
   "metadata": {},
   "source": [
    "### Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e752dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/age.csv')\n",
    "age_sample = sample_categorical_census(df,\n",
    "                                       'Age (6 categories)',\n",
    "                                       'Observation',\n",
    "                                       [])\n",
    "print(age_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b93502",
   "metadata": {},
   "source": [
    "### Elderly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124dfbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "elderly_sample = 1 if age_sample == 'Aged 65 years and over' else 0\n",
    "print(elderly_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f070dc",
   "metadata": {},
   "source": [
    "### Children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57649f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "child_sample = 1 if age_sample == 'Aged 15 years and under' else 0\n",
    "print(child_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac73470b",
   "metadata": {},
   "source": [
    "### Employment history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f254f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Does not apply: Either child or in employment\n",
    "'''\n",
    "\n",
    "employ = pd.read_csv('data/employment-age.csv')\n",
    "filtered_employ = employ[employ['Age (6 categories)'] == age_sample]\n",
    "\n",
    "employ_sample = sample_categorical_census(filtered_employ,\n",
    "                                       'Employment history (4 categories)',\n",
    "                                       'Observation',\n",
    "                                       [])\n",
    "\n",
    "print(employ_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c5c3fd",
   "metadata": {},
   "source": [
    "### Highest level of qualification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa60d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Does not apply is only for <15, \n",
    "'''\n",
    "qual = pd.read_csv('data/qualification-age.csv')\n",
    "filtered_qual = qual[qual['Age (6 categories)'] == age_sample]\n",
    "\n",
    "qual_sample = sample_categorical_census(filtered_qual,\n",
    "                                       'Highest level of qualification (7 categories)',\n",
    "                                       'Observation',\n",
    "                                       [])\n",
    "print(qual_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0d3246",
   "metadata": {},
   "source": [
    "### Lifestage of household reference person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a93cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lifestage = pd.read_csv('data/lifestage_hrp_age.csv')\n",
    "filtered_lifestage = lifestage[lifestage['Age (6 categories)'] == age_sample]\n",
    "lifestage_sample = sample_categorical_census(filtered_lifestage,\n",
    "                                       'Lifestage of Household Reference Person(13 categories)',\n",
    "                                       'Observation',\n",
    "                                       ['Does not apply'])\n",
    "print(lifestage_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3940b54",
   "metadata": {},
   "source": [
    "### Accomodation type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7290932",
   "metadata": {},
   "outputs": [],
   "source": [
    "acco_type = pd.read_csv('data/accomodation_type.csv')\n",
    "acco_type_sample = sample_categorical_census(acco_type,\n",
    "                                       'Accommodation type (5 categories)',\n",
    "                                       'Observation',\n",
    "                                       [])\n",
    "print(acco_type_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd1facc",
   "metadata": {},
   "source": [
    "### Vehicle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977ee25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicle = pd.read_csv('data/vehicle.csv')\n",
    "vehicle_sample = sample_categorical_census(vehicle,\n",
    "                                       'Car or van availability (3 categories)',\n",
    "                                       'Observation',\n",
    "                                       ['Does not apply'])\n",
    "print(vehicle_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac8b150",
   "metadata": {},
   "source": [
    "### Second address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0464c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "second_add = pd.read_csv('data/second_address.csv')\n",
    "second_add_sample = sample_categorical_census(second_add,\n",
    "                                       'Second address indicator (3 categories)',\n",
    "                                       'Observation',\n",
    "                                       [])\n",
    "print(second_add_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1c3eb3",
   "metadata": {},
   "source": [
    "### Household size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e6d33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "house_size = pd.read_csv('data/household_size.csv')\n",
    "house_size_sample = sample_categorical_census(house_size,\n",
    "                                       'Household size (5 categories)',\n",
    "                                       'Observation',\n",
    "                                       ['0 people in household'])\n",
    "print(house_size_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b358c54",
   "metadata": {},
   "source": [
    "### Economic activity status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406de34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Does not apply is only for <15, \n",
    "'''\n",
    "eas = pd.read_csv('data/nssec_economic_age.csv')\n",
    "filtered_eas = eas[eas['Age (6 categories)'] == age_sample]\n",
    "\n",
    "eas_sample = sample_categorical_census(filtered_eas,\n",
    "                                       'Economic activity status (4 categories)',\n",
    "                                       'Observation',\n",
    "                                       [])\n",
    "print(eas_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f2ac37",
   "metadata": {},
   "source": [
    "### Ns-SeC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a15cb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "nssec = pd.read_csv('data/nssec_economic_age.csv')\n",
    "filtered_nssec = nssec[nssec['Age (6 categories)'] == age_sample]\n",
    "filtered_nssec = filtered_nssec[filtered_nssec['Economic activity status (4 categories)'] == eas_sample]\n",
    "nssec_sample = sample_categorical_census(filtered_nssec, \n",
    "                                         'National Statistics Socio-economic Classification (NS-SeC) (10 categories)', \n",
    "                                         'Observation',\n",
    "                                         [])\n",
    "print(nssec_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92244d55",
   "metadata": {},
   "source": [
    "### Mean income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3ccc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "income = pd.read_csv(\"data/mean_income.csv\")\n",
    "income['Total annual income (£)'] = (\n",
    "    income['Total annual income (£)']\n",
    "    .str.strip()        \n",
    "    .str.replace(',', '')      \n",
    "    .astype(float)       \n",
    ")\n",
    "\n",
    "log_income = np.log(income['Total annual income (£)'])\n",
    "shape, loc, scale = skewnorm.fit(log_income)\n",
    "sample_log = skewnorm.rvs(shape, loc=loc, scale=scale, size=1)\n",
    "income_sample = np.exp(sample_log)[0]\n",
    "\n",
    "NSSEC = {\n",
    "    \"L1, L2 and L3: Higher managerial, administrative and professional occupations\": 1.90,\n",
    "    \"L4, L5 and L6: Lower managerial, administrative and professional occupations\": 1.35,\n",
    "    \"L7: Intermediate occupations\": 1.00,\n",
    "    \"L8 and L9: Small employers and own account workers\": 1.10,\n",
    "    \"L10 and L11: Lower supervisory and technical occupations\": 0.90,\n",
    "    \"L12: Semi-routine occupations\": 0.75,\n",
    "    \"L13: Routine occupations\": 0.65,\n",
    "    \"L14.1 and L14.2: Never worked and long-term unemployed\": 0.40,\n",
    "    \"L15: Full-time students\": 0.35,\n",
    "    \"Does not apply\": 0.00\n",
    "}\n",
    "\n",
    "income_sample *= NSSEC[nssec_sample]\n",
    "print(income_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9351d79c",
   "metadata": {},
   "source": [
    "### Low-income fraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bada3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "median = income['Total annual income (£)'].median()\n",
    "low_income_threshold = 0.6 * median\n",
    "print(low_income_threshold)\n",
    "\n",
    "low_income_sample = 1 if income_sample < low_income_threshold else 0\n",
    "print(low_income_sample)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d78a568",
   "metadata": {},
   "source": [
    "### No. adults employed in household"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e57113",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_adults = pd.read_csv('data/household_employed_size.csv')\n",
    "filtered_num_adults = num_adults[num_adults['Household size (5 categories)'] == house_size_sample]\n",
    "\n",
    "num_adults_sample = sample_categorical_census(filtered_num_adults,\n",
    "                                       'Number of adults in employment in household (5 categories)',\n",
    "                                       'Observation',\n",
    "                                       ['Does not apply'])\n",
    "print(num_adults_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979a5684",
   "metadata": {},
   "source": [
    "### No. disabled people household "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad57797",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_disable = pd.read_csv('data/household_disabled_size.csv')\n",
    "filtered_num_disable = num_disable[num_disable['Household size (5 categories)'] == house_size_sample]\n",
    "\n",
    "num_disable_sample = sample_categorical_census(filtered_num_disable,\n",
    "                                       'Number of disabled people in household (4 categories)',\n",
    "                                       'Observation',\n",
    "                                       ['Does not apply'])\n",
    "print(num_disable_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9720fd26",
   "metadata": {},
   "source": [
    "### No. long-term health in household"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e7dcf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_long = pd.read_csv('data/household_long-term_size.csv')\n",
    "filtered_num_long = num_long[num_long['Household size (5 categories)'] == house_size_sample]\n",
    "\n",
    "num_long_sample = sample_categorical_census(filtered_num_long,\n",
    "                                       'Number of people in household with a long-term heath condition but are not disabled (4 categories)',\n",
    "                                       'Observation',\n",
    "                                       ['Does not apply'])\n",
    "print(num_long_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ea9a4f",
   "metadata": {},
   "source": [
    "### Deprived in education"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a83df93",
   "metadata": {},
   "outputs": [],
   "source": [
    "dep_edu = pd.read_csv('data/deprived_education+deps.csv')\n",
    "filtered_dep_edu = dep_edu[dep_edu['Highest level of qualification (7 categories)'] == qual_sample]\n",
    "\n",
    "dep_edu_sample = sample_categorical_census(filtered_dep_edu,\n",
    "                                       'Household deprived in the education dimension (3 categories)',\n",
    "                                       'Observation',\n",
    "                                       ['Does not apply'])\n",
    "print(dep_edu_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cc1c86",
   "metadata": {},
   "source": [
    "### Deprived in employment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb62d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "dep_employ = pd.read_csv('data/deprived_employment+deps.csv')\n",
    "filtered_dep_employ = dep_employ[dep_employ['Employment history (4 categories)'] == employ_sample]\n",
    "filtered_dep_employ = filtered_dep_employ[filtered_dep_employ['National Statistics Socio-economic Classification (NS-SeC) (10 categories)'] == nssec_sample]\n",
    "dep_employ_sample = sample_categorical_census(filtered_dep_employ,\n",
    "                                       'Household deprived in the employment dimension (3 categories)',\n",
    "                                       'Observation',\n",
    "                                       [])\n",
    "print(dep_employ_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515c00cf",
   "metadata": {},
   "source": [
    "### Deprived in health and disability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aab6d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "dep_health = pd.read_csv('data/deprived_health+deps.csv')\n",
    "filtered_dep_health = dep_health[dep_health['Number of people in household with a long-term heath condition but are not disabled (4 categories)'] == num_long_sample]\n",
    "filtered_dep_health = filtered_dep_health[filtered_dep_health['Number of disabled people in household (4 categories)'] == num_disable_sample]\n",
    "dep_health_sample = sample_categorical_census(filtered_dep_health,\n",
    "                                       'Household deprived in the health and disability dimension (3 categories)',\n",
    "                                       'Observation',\n",
    "                                       [])\n",
    "print(dep_health_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35967c9d",
   "metadata": {},
   "source": [
    "### No. of people per room in household "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea33c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_people = pd.read_csv('data/people_per_room_hsize.csv')\n",
    "filtered_num_people = num_people[num_people['Household size (5 categories)'] == house_size_sample]\n",
    "num_people_sample = sample_categorical_census(filtered_num_people,\n",
    "                                       'Number of people per room in household (5 categories)',\n",
    "                                       'Observation',\n",
    "                                       ['Does not apply'])\n",
    "print(num_people_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1a2168",
   "metadata": {},
   "source": [
    "### Occupancy rating for rooms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7e6a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "occupancy = pd.read_csv('data/occupancy_rating_nopeopleper.csv')\n",
    "filtered_occupancy = occupancy[occupancy['Number of people per room in household (5 categories)'] == num_people_sample]\n",
    "num_occupancy = sample_categorical_census(filtered_occupancy,\n",
    "                                       'Occupancy rating for rooms (5 categories)',\n",
    "                                       'Observation',\n",
    "                                       ['Does not apply'])\n",
    "print(num_occupancy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557d5f2a",
   "metadata": {},
   "source": [
    "### Deprived in housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccafc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "dep_housing = pd.read_csv('data/deprived_housing+deps.csv')\n",
    "filtered_dep_housing = dep_housing[dep_housing['Number of people per room in household (5 categories)'] == num_people_sample]\n",
    "filtered_dep_housing = filtered_dep_housing[filtered_dep_housing['Occupancy rating for rooms (5 categories)'] == num_occupancy]\n",
    "dep_housing_sample = sample_categorical_census(filtered_dep_housing,\n",
    "                                       'Household deprived in the housing dimension (3 categories)',\n",
    "                                       'Observation',\n",
    "                                       ['Does not apply'])\n",
    "print(dep_housing_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31f14dc",
   "metadata": {},
   "source": [
    "### Household deprivation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb74312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smaller = less deprived\n",
    "household_dep_sample = (dep_edu_sample == 'Household is deprived in the education dimension') + (dep_employ_sample == 'Household is deprived in the employment dimension') + (dep_health_sample == 'Household is deprived in the health and disability dimension') + (dep_housing_sample == 'Household is deprived in the housing dimension')\n",
    "print(household_dep_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bb8554",
   "metadata": {},
   "source": [
    "### Tenure of household"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0099bb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "tenure = pd.read_csv('data/tenure.csv')\n",
    "tenure_sample = sample_categorical_census(tenure,\n",
    "                                       'Tenure of household (7 categories)',\n",
    "                                       'Observation',\n",
    "                                       ['Does not apply'])\n",
    "print(tenure_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52373c6",
   "metadata": {},
   "source": [
    "### Household access to internet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b324613d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if lifestage_sample == 'Household reference person is aged 66 years or over: One-person household':\n",
    "    prob = 0.85\n",
    "else:\n",
    "    prob = 0.98\n",
    "internet_sample = np.random.binomial(n=1, p=prob)\n",
    "print(internet_sample)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a4547f",
   "metadata": {},
   "source": [
    "### Home insurance coverage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3db6290",
   "metadata": {},
   "outputs": [],
   "source": [
    "home_insure = 0.75\n",
    "home_insure_sample = np.random.binomial(1, home_insure, size=1)[0]\n",
    "print(home_insure_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be8d018",
   "metadata": {},
   "source": [
    "### Health insurance coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bb1bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "health_insure = 0.14\n",
    "health_insure_sample = np.random.binomial(1, health_insure, size=1)[0]\n",
    "print(health_insure_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd071b6",
   "metadata": {},
   "source": [
    "### Household (INCORPORATE INSURANCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544b615b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Home ownership:\n",
    "- wealth accumulation\n",
    "- tenure security\n",
    "Mortgage holders retain equity but face some financial exposure\n",
    "Private renters:\n",
    "- higher housing cost volatility\n",
    "- lower security\n",
    "Social renters:\n",
    "- low-income\n",
    "- benefit-dependent\n",
    "- high-deprivation populations\n",
    "'''\n",
    "TENURE_RISK = {\n",
    "    \"Owned: Owns outright\": 0.0,\n",
    "    \"Owned: Owns with a mortgage or loan or shared ownership\": 0.1,\n",
    "    \"Private rented: Private landlord or letting agency\": 0.6,\n",
    "    \"Private rented: Other private rented or lives rent free\": 0.6,\n",
    "    \"Social rented: Rents from council or Local Authority\": 0.8,\n",
    "    \"Social rented: Other social rented\": 0.8,\n",
    "}\n",
    "\n",
    "'''\n",
    "- Housing quality, space, and permanence decrease down the list\n",
    "- Flats and temporary housing show higher overcrowding and energy risk\n",
    "- Temporary structures are near-maximal risk \n",
    "'''\n",
    "ACCO_RISK = {\n",
    "    \"Whole house or bungalow: Detached\": 0.1,\n",
    "    \"Whole house or bungalow: Semi-detached\": 0.1,\n",
    "    \"Whole house or bungalow: Terraced\": 0.3,\n",
    "    \"Flat, maisonette or apartment\": 0.5,\n",
    "    \"A caravan or other mobile or temporary structure\": 0.9,\n",
    "}\n",
    "\n",
    "'''\n",
    "Single-person households:\n",
    "- income fragility\n",
    "- social isolation risk\n",
    "Two-person households:\n",
    "- risk-sharing\n",
    "Large households:\n",
    "- crowding\n",
    "- higher costs\n",
    "- child dependency\n",
    "'''\n",
    "SIZE_RISK = {\n",
    "    \"1 person in household\": 0.5,\n",
    "    \"2 people in household\": 0.2,\n",
    "    \"3 people in household\": 0.3,\n",
    "    \"4 or more people in household\": 0.6,\n",
    "}\n",
    "\n",
    "'''\n",
    "Internet access for weather warning services\n",
    "'''\n",
    "INTERNET_RISK = {\n",
    "    1: 0.0,\n",
    "    0: 0.6,\n",
    "}\n",
    "\n",
    "'''\n",
    "Risk increases with less earners\n",
    "'''\n",
    "EMPLOYMENT_RISK = {\n",
    "    \"3 or more adults in employment in household\": 0.0,\n",
    "    \"2 adults in employment in household\": 0.2,\n",
    "    \"1 adult in employment in household\": 0.5,\n",
    "    \"No adults in employment in household\": 0.9,\n",
    "}\n",
    "\n",
    "'''\n",
    "Deprivation in education, employment, health, housing\n",
    "'''\n",
    "DEPRIVATION_RISK = {\n",
    "    0: 0.0,\n",
    "    1: 0.25,\n",
    "    2: 0.5,\n",
    "    3: 0.75,\n",
    "    4: 1.0,\n",
    "}\n",
    "\n",
    "'''\n",
    "Older single households:\n",
    "- health risks\n",
    "- lower chance of internet access\n",
    "- elderly vulnerable\n",
    "Households with dependent children:\n",
    "- cost pressure\n",
    "- children vulnerable\n",
    "Child-free working-age households are most resilient\n",
    "'''\n",
    "def lifestage_risk_map(x):\n",
    "    if \"66 years or over\" in x and \"One-person\" in x:\n",
    "        return 0.7\n",
    "    if \"Dependent children\" in x:\n",
    "        return 0.6\n",
    "    if \"Two or more person household: No dependent children\" in x:\n",
    "        return 0.3\n",
    "    return 0.4\n",
    "\n",
    "tenure_risk = TENURE_RISK[tenure_sample]\n",
    "acco_risk = ACCO_RISK[acco_type_sample]\n",
    "size_risk = SIZE_RISK[house_size_sample]\n",
    "internet_risk = INTERNET_RISK[internet_sample]\n",
    "employment_risk = EMPLOYMENT_RISK[num_adults_sample]\n",
    "deprivation_risk = DEPRIVATION_RISK[household_dep_sample]\n",
    "lifestage_risk = lifestage_risk_map(lifestage_sample)\n",
    "\n",
    "'''\n",
    "Deprivation: looks at 4 factors (higher weight)\n",
    "Employment: Income stability\n",
    "Employment: Wealth and security\n",
    "Others: Secondary modifiers\n",
    "'''\n",
    "WEIGHTS = {\n",
    "    \"tenure_risk\": 0.15,\n",
    "    \"acco_risk\": 0.10,\n",
    "    \"size_risk\": 0.10,\n",
    "    \"internet_risk\": 0.10,\n",
    "    \"lifestage_risk\": 0.10,\n",
    "    \"employment_risk\": 0.20,\n",
    "    \"deprivation_risk\": 0.25,\n",
    "}\n",
    " \n",
    "risk_score = (\n",
    "    WEIGHTS[\"tenure_risk\"]       * tenure_risk +\n",
    "    WEIGHTS[\"acco_risk\"]         * acco_risk +\n",
    "    WEIGHTS[\"size_risk\"]         * size_risk +\n",
    "    WEIGHTS[\"internet_risk\"]     * internet_risk +\n",
    "    WEIGHTS[\"lifestage_risk\"]    * lifestage_risk +\n",
    "    WEIGHTS[\"employment_risk\"]   * employment_risk +\n",
    "    WEIGHTS[\"deprivation_risk\"]  * deprivation_risk\n",
    ")\n",
    "\n",
    "print('Tenure -', tenure_sample)\n",
    "print('Accomodation type -', acco_type_sample)\n",
    "print('Household size -', house_size_sample)\n",
    "print('Internet access -', internet_sample)\n",
    "print('Adults employed -', num_adults_sample)\n",
    "print('Household deprivation -', household_dep_sample)\n",
    "print('Lifestage HRP -', lifestage_sample)\n",
    "\n",
    "noise = np.random.normal(loc=0, scale=0.03) \n",
    "household_risk_score = np.clip(risk_score + noise, 0, 1)\n",
    "print(household_risk_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f1f846",
   "metadata": {},
   "source": [
    "## Derived"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c7d539",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684f3dbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
