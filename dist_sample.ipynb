{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11896ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from scipy.stats import gamma, lognorm, norm, skewnorm, truncnorm\n",
    "import random\n",
    "import os\n",
    "import rasterio\n",
    "import glob\n",
    "import datetime as dt\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "def sample_beta(df: pd.DataFrame, category_col: str, a: str, b: str, value_col: str):\n",
    "    total = df.groupby(category_col)[value_col].sum()\n",
    "\n",
    "    alpha = total[a] + 1\n",
    "    beta = total[b] + 1\n",
    "\n",
    "    sample = np.random.beta(alpha, beta, size=1)[0]\n",
    "    return sample\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a6d969",
   "metadata": {},
   "source": [
    "## Hydrological"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb626db2",
   "metadata": {},
   "source": [
    "### 24-Hour Precipitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67abdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "precipitation = pd.read_csv(\"data/precipitation.csv\")\n",
    "rainfall = precipitation[\"rainfall\"]\n",
    "pct_zero = (rainfall == 0).sum() / len(rainfall)\n",
    "zero_sample = np.random.binomial(n=1, p=pct_zero, size=1)[0]\n",
    "\n",
    "precipitation_nonzero = precipitation[precipitation[\"rainfall\"] != 0].copy()\n",
    "rainfall_nonzero = precipitation[\"rainfall\"]\n",
    "max_prec = max(rainfall_nonzero)\n",
    "shape, loc, scale = gamma.fit(rainfall_nonzero)\n",
    "prec_sample = 0 if zero_sample == 1 else gamma.rvs(a=shape, loc=loc, scale=scale, size=1)[0]\n",
    "print(prec_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fd2f4a",
   "metadata": {},
   "source": [
    "### Groundwater level (WORK IN PREC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ef57c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose random groundwater station, Gamma\n",
    "\n",
    "# mAOD\n",
    "# add prec dependency, modify ground water level based on previous prec_sample\n",
    "\n",
    "groundwater_files = [f for f in os.listdir(\"data/groundwater_level\") if f.lower().endswith(\".csv\")]\n",
    "gw_file = random.choice(groundwater_files)\n",
    "\n",
    "path = os.path.join(\"data/groundwater_level\", gw_file)\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "values = pd.to_numeric(df[\"value\"], errors=\"coerce\").dropna()\n",
    "a, loc, scale = skewnorm.fit(values)\n",
    "gw_sample = skewnorm.rvs(a, loc, scale, size=1)[0]\n",
    "\n",
    "print(f\"Selected file: {gw_file}\")\n",
    "print(gw_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba97ce24",
   "metadata": {},
   "source": [
    "### River flow (WORK IN PREC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a3a8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random choose river station, Gamma\n",
    "\n",
    "# m3/s\n",
    "# add prec dependency, modify ground water level based on previous prec_sample\n",
    "\n",
    "river_flow_files = [f for f in os.listdir(\"data/river_flow\") if f.lower().endswith(\".csv\")]\n",
    "rf_file = random.choice(river_flow_files)\n",
    "\n",
    "path = os.path.join(\"data/river_flow\", rf_file)\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "values = pd.to_numeric(df[\"value\"], errors=\"coerce\").dropna()\n",
    "shape, loc, scale = gamma.fit(values, floc=0)\n",
    "rf_sample = gamma.rvs(a=shape, loc=loc, scale=scale, size=1)[0]\n",
    "\n",
    "print(f\"Selected file: {rf_file}\")\n",
    "print(rf_sample)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f12bb7",
   "metadata": {},
   "source": [
    "### River level (WORK IN PREC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8697fe6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using same river station as river flow, Gamma\n",
    "\n",
    "# m\n",
    "# add prec dependency, modify ground water level based on previous prec_sample\n",
    "\n",
    "file_prefix = rf_file.split('-')[0]\n",
    "river_level_files = [f for f in os.listdir(\"data/river_level\") if f.lower().endswith(\".csv\")]\n",
    "for f in river_level_files:\n",
    "    if file_prefix in f:\n",
    "        rl_file = f\n",
    "        break\n",
    "    \n",
    "path = os.path.join(\"data/river_level\", rl_file)\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "values = pd.to_numeric(df[\"value\"], errors=\"coerce\").dropna()\n",
    "shape, loc, scale = gamma.fit(values, floc=0)\n",
    "rl_sample = gamma.rvs(a=shape, loc=loc, scale=scale, size=1)[0]\n",
    "\n",
    "print(f\"Selected file: {rl_file}\")\n",
    "print(rl_sample)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e530fa6",
   "metadata": {},
   "source": [
    "## Other"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb0199d",
   "metadata": {},
   "source": [
    "### Urban/Rural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec72cb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "urban = pd.read_csv(\"data/urban_rural.csv\")\n",
    "counts = urban['Urban_rural_flag'].value_counts()\n",
    "urban_probs = counts / counts.sum()\n",
    "p_urban = urban_probs['Urban']\n",
    "urban_sample = np.random.binomial(n=1, p=p_urban, size=1)[0]\n",
    "print(urban_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132cb0df",
   "metadata": {},
   "source": [
    "### Population density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6786f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "popden = pd.read_csv(\"data/population_density.csv\", dtype={\"LAD2021\": \"string\", \"OA21CD\": \"string\", \"Total\": \"Int64\"})\n",
    "merged = popden.merge(urban, on=\"OA21CD\", how=\"left\")\n",
    "popden_urbanrural = merged[['LAD2021','OA21CD','Total','Urban_rural_flag']]\n",
    "\n",
    "flag = 'urban' if urban_sample == 1 else 'rural'\n",
    "popden_total  = popden_urbanrural[popden_urbanrural[\"Urban_rural_flag\"].str.lower() == flag][\"Total\"]\n",
    "\n",
    "log = np.log(popden_total)\n",
    "\n",
    "mu, sigma = log.mean(), log.std()\n",
    "\n",
    "popden_sample = np.random.lognormal(mean=mu, sigma=sigma, size=1)[0]\n",
    "print(popden_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8b737c",
   "metadata": {},
   "source": [
    "### Mean property value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4faa335f",
   "metadata": {},
   "outputs": [],
   "source": [
    "property = pd.read_csv(\"data/property_value.csv\")\n",
    "property = property.dropna(subset=['price', 'property_type', 'duration'])\n",
    "property = property[property['price'] > 0]\n",
    "\n",
    "log_property = np.log(property['price'])\n",
    "shape, loc, scale = skewnorm.fit(log_property)\n",
    "sample_log = skewnorm.rvs(shape, loc=loc, scale=scale, size=1)\n",
    "property_sample = np.exp(sample_log)[0]\n",
    "print(property_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a622c9",
   "metadata": {},
   "source": [
    "### Building age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb75b959",
   "metadata": {},
   "outputs": [],
   "source": [
    "building_age = pd.read_csv(\"data/property_age.csv\")\n",
    "age_columns = [\n",
    "    \"BP_PRE_1900\",\"BP_1900_1918\",\"BP_1919_1929\",\"BP_1930_1939\",\n",
    "    \"BP_1945_1954\",\"BP_1955_1964\",\"BP_1965_1972\",\"BP_1973_1982\",\n",
    "    \"BP_1983_1992\",\"BP_1993_1999\",\"BP_2000_2009\",\"BP_2010_2015\"\n",
    "]\n",
    "age_totals = building_age[age_columns].sum()\n",
    "age_probs = age_totals / age_totals.sum()\n",
    "age_categories = age_totals.index.tolist()\n",
    "building_range = np.random.choice(age_categories, size=1, p=age_probs)[0]\n",
    "if building_range == \"BP_PRE_1900\":\n",
    "    building_age_sample = 1900\n",
    "else:\n",
    "    a, b = building_range.split('_')[1:]\n",
    "    building_age_sample = np.random.randint(int(a), int(b)+1)\n",
    "print(building_range)\n",
    "print(building_age_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a1445c",
   "metadata": {},
   "source": [
    "### Season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febedd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "season = ['Winter', 'Spring', 'Summer', 'Autumn']\n",
    "season_sample = random.choice(season)\n",
    "print(season_sample)\n",
    "\n",
    "SEASON_MONTHS = {\n",
    "    \"Spring\": [3, 4, 5],\n",
    "    \"Summer\": [6, 7, 8],\n",
    "    \"Autumn\": [9, 10, 11],\n",
    "    \"Winter\": [12, 1, 2],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7abfa9",
   "metadata": {},
   "source": [
    "### Holiday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c8d2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Min 28 days off annually\n",
    "'''\n",
    "p = 28 / 365\n",
    "holiday_binary_sample = 1 if random.random() < p else 0\n",
    "print(holiday_binary_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f8daca",
   "metadata": {},
   "source": [
    "### Emergency Response Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd104f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = pd.read_csv(\"data/response_times.csv\")\n",
    "def hhmmss_to_hours(s):\n",
    "    h, m, sec = map(int, s.split(\":\"))\n",
    "    return h + m/60 + sec/3600\n",
    "\n",
    "SEASON_MONTHS_FULL = {\n",
    "    \"Spring\": [\"March\", \"April\", \"May\"],\n",
    "    \"Summer\": [\"June\", \"July\", \"August\"],\n",
    "    \"Autumn\": [\"September\", \"October\", \"November\"],\n",
    "    \"Winter\": [\"December\", \"January\", \"February\"],\n",
    "}\n",
    "months = SEASON_MONTHS_FULL[season_sample]\n",
    "response_season = response[response['month'].isin(months)]\n",
    "\n",
    "c2_mean = response_season[\"C2_mean\"].apply(hhmmss_to_hours)\n",
    "c3_mean = response_season[\"C3_mean\"].apply(hhmmss_to_hours)\n",
    "c2_count = response_season['C2_count'].str.replace(',', '').astype(int).sum()\n",
    "c3_count = response_season['C3_count'].str.replace(',', '').astype(int).sum()\n",
    "c2_prob = c2_count / (c2_count + c3_count)\n",
    "c3_prob = c3_count / (c2_count + c3_count)\n",
    "response_category = np.random.choice(['C2', 'C3'], size=1, p=[c2_prob, c3_prob])[0]\n",
    "if response_category == 'C2':\n",
    "    shape, loc, scale = lognorm.fit(c2_mean, floc=0)\n",
    "else:\n",
    "    shape, loc, scale = lognorm.fit(c3_mean, floc=0)\n",
    "response_sample = lognorm.rvs(shape, loc=loc, scale=scale, size=1)[0]\n",
    "\n",
    "popden_mean = popden_total.mean()\n",
    "if popden_sample < popden_mean: # scale based on average population density\n",
    "    popden_factor = 1 - 0.5 * ((popden_mean - popden_sample) / popden_mean)  # reduce scale\n",
    "else:\n",
    "    popden_factor = 1 + 0.5 * ((popden_sample - popden_mean) / popden_mean)\n",
    "prec_factor = np.exp(prec_sample / 50) # small rain = minimal effect, heavy rain = large effect\n",
    "print(popden_factor, prec_factor)\n",
    "adjusted_response = response_sample * popden_factor * prec_factor\n",
    "print(adjusted_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec6153d",
   "metadata": {},
   "source": [
    "### Ambulance handover delays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a446029",
   "metadata": {},
   "outputs": [],
   "source": [
    "handover = pd.read_csv(\"data/ambulance_handover.csv\")\n",
    "\n",
    "for col in [\"Handover time known\", \"Over 15 minutes\", \"Over 30 minutes\", \"Over 60 minutes\", \"Handover time unknown\", \"All handovers\"]:\n",
    "    handover[col] = handover[col].str.replace(\",\", \"\").astype(int)\n",
    "\n",
    "handover[\"Under 15 min\"] = handover[\"Handover time known\"] - handover[\"Over 15 minutes\"]\n",
    "handover[\"15–30 min\"] = handover[\"Over 15 minutes\"] - handover[\"Over 30 minutes\"]\n",
    "handover[\"30–60 min\"] = handover[\"Over 30 minutes\"] - handover[\"Over 60 minutes\"]\n",
    "handover[\"Over 60 min\"] = handover[\"Over 60 minutes\"]\n",
    "\n",
    "handover[\"Date_parsed\"] = pd.to_datetime(handover[\"Date\"], format=\"%b'%y\")\n",
    "handover_season = handover[handover[\"Date_parsed\"].dt.month.isin(SEASON_MONTHS[season_sample])]\n",
    "\n",
    "counts = handover_season[[\"Under 15 min\", \"15–30 min\", \"30–60 min\", \"Over 60 min\"]].sum()\n",
    "probs = counts / counts.sum()\n",
    "\n",
    "def trunc_normal(low, high, mean, sd):\n",
    "    a, b = (low - mean) / sd, (high - mean) / sd\n",
    "    return truncnorm.rvs(a, b, loc=mean, scale=sd)\n",
    "\n",
    "def sample_time_from_band(band):\n",
    "    if band == \"Under 15 min\":\n",
    "        return trunc_normal(0, 15, mean=8, sd=3)\n",
    "\n",
    "    if band == \"15–30 min\":\n",
    "        return trunc_normal(15, 30, mean=22, sd=4)\n",
    "\n",
    "    if band == \"30–60 min\":\n",
    "        return trunc_normal(30, 60, mean=45, sd=7)\n",
    "\n",
    "    if band == \"Over 60 min\":\n",
    "        return 60 + np.random.exponential(scale=20)\n",
    "    raise ValueError(f\"Unknown band: {band}\")\n",
    "\n",
    "def sample_handover_time(probs, n=1):\n",
    "    bands = probs.index.to_numpy()\n",
    "    p = probs.to_numpy()\n",
    "\n",
    "    sampled_bands = np.random.choice(bands, size=n, p=p)\n",
    "    print(sampled_bands)\n",
    "\n",
    "    times = np.array([sample_time_from_band(b) for b in sampled_bands])\n",
    "    return times if n > 1 else times[0]\n",
    "\n",
    "handover_time_sample = sample_handover_time(probs)\n",
    "print(handover_time_sample)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383c6272",
   "metadata": {},
   "source": [
    "### Hospital bed availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4959703e",
   "metadata": {},
   "outputs": [],
   "source": [
    "beds = pd.read_csv(\"data/hospital_beds.csv\")\n",
    "\n",
    "beds[\"Period\"] = pd.to_datetime(beds[\"Period\"], format=\"%d/%m/%Y\")\n",
    "# Aggregate by month (sum across all hospitals)\n",
    "beds = beds.groupby('Period')[['Available', 'Occupied', 'Free']].sum().reset_index()\n",
    "\n",
    "# Compute occupancy percentage\n",
    "beds['OccupancyPct'] = beds['Occupied'] / beds['Available']\n",
    "\n",
    "beds_season = beds[beds[\"Period\"].dt.month.isin(SEASON_MONTHS[season_sample])]\n",
    "values = beds_season[\"OccupancyPct\"].dropna().values\n",
    "\n",
    "alpha = beds_season['Free'].sum() + 1\n",
    "beta = beds_season['Occupied'].sum() + 1\n",
    "\n",
    "beds_sample = np.random.beta(alpha, beta, size=1)[0]\n",
    "print(beds_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51942825",
   "metadata": {},
   "source": [
    "## Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8da0865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from overall grid shapefile\n",
    "gdf = gpd.read_file('data/grid/grid.shp')\n",
    "features = [\n",
    "    \"water_dens\", \"water_dist\", \"risk_score\", \"elevation\",\n",
    "    \"impervious\", \"historic\", \"road_dens\", \"road_dist\", \"hospital\"\n",
    "]\n",
    "\n",
    "def sample_cell_with_noise(gdf, features, noise_scale=0.05):\n",
    "    row = gdf.sample(1).iloc[0]\n",
    "    sample = {}\n",
    "    for f in features:\n",
    "        val = row[f]\n",
    "        noise = np.random.normal(0, noise_scale * abs(val + 1e-6))\n",
    "        sample[f] = max(val + noise, 0)\n",
    "    sample[\"geometry\"] = row.geometry \n",
    "    return sample\n",
    "\n",
    "def sample_neighborhood(gdf, features, k=5):\n",
    "    cell = gdf.sample(1)\n",
    "    dists = gdf.geometry.distance(cell.geometry.iloc[0])\n",
    "    neighbors = gdf.loc[dists.nsmallest(k).index]\n",
    "    return neighbors[features].mean().to_dict()\n",
    "\n",
    "grid_sample = sample_cell_with_noise(gdf, features)\n",
    "neighborhood_sample = sample_neighborhood(gdf, features)\n",
    "print(grid_sample)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1567f536",
   "metadata": {},
   "source": [
    "### Soil moisture saturation (WORK IN PREC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868df05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose pixel that contains the overall grid cell chosen\n",
    "# sample from normal distribution of time-series values for the pixel\n",
    "# filters by season\n",
    "\n",
    "cell_geom = grid_sample[\"geometry\"]  # polygon of the sampled grid cell\n",
    "\n",
    "# Load soil moisture rasters\n",
    "tiff_files = sorted(glob.glob(\"data/soil_moisture/*.tif\"))\n",
    "season_tiffs = []\n",
    "\n",
    "for f in tiff_files:\n",
    "    # Extract date from filename: dt_smuk_2023-12-22.tif\n",
    "    date_str = f.split(\"_\")[-1].replace(\".tif\", \"\")\n",
    "    file_date = dt.datetime.strptime(date_str, \"%Y-%m-%d\")\n",
    "    if file_date.month in SEASON_MONTHS[season_sample]:\n",
    "        season_tiffs.append(f)\n",
    "\n",
    "# Load soil moisture rasters\n",
    "stack = []\n",
    "with rasterio.open(season_tiffs[0]) as src:\n",
    "    transform = src.transform\n",
    "    nodata = src.nodata\n",
    "    for f in season_tiffs:\n",
    "        with rasterio.open(f) as s:\n",
    "            data = s.read(1).astype(np.float32)\n",
    "            if nodata is not None:\n",
    "                data[data == nodata] = np.nan\n",
    "            stack.append(data)\n",
    "\n",
    "stack = np.stack(stack, axis=0)  # shape: (time, rows, cols)\n",
    "\n",
    "# Find the single pixel containing the centroid of the sampled grid cell\n",
    "centroid_x, centroid_y = cell_geom.centroid.x, cell_geom.centroid.y\n",
    "col, row = ~transform * (centroid_x, centroid_y)\n",
    "row, col = int(row), int(col)\n",
    "\n",
    "# Ensure row/col are within raster bounds\n",
    "row = np.clip(row, 0, stack.shape[1]-1)\n",
    "col = np.clip(col, 0, stack.shape[2]-1)\n",
    "\n",
    "# Extract time series for that pixel\n",
    "values = stack[:, row, col]\n",
    "values = values[~np.isnan(values)]\n",
    "\n",
    "# Fit normal distribution safely\n",
    "if len(values) == 0:\n",
    "    soil_sample = np.nan\n",
    "else:\n",
    "    mu, sigma = norm.fit(values)\n",
    "    sigma = max(sigma, 1e-6)\n",
    "    soil_sample = norm.rvs(mu, sigma)\n",
    "\n",
    "print(soil_sample)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96885526",
   "metadata": {},
   "source": [
    "### Flood depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c785ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "clipped_data = {}\n",
    "\n",
    "for shp_path in glob.glob('data/flood_risk/*/*.shp'):\n",
    "    gdf_shp = gpd.read_file(shp_path)\n",
    "    gdf_shp_clipped = gdf_shp.clip(grid_sample[\"geometry\"].bounds)\n",
    "    \n",
    "    # Store clipped data\n",
    "    key = os.path.basename(shp_path).replace(\".shp\",\"\")\n",
    "    clipped_data[key] = gdf_shp_clipped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa9a529",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_geom = gpd.GeoSeries([grid_sample[\"geometry\"]])\n",
    "if grid_geom.crs is None:\n",
    "    grid_geom = grid_geom.set_crs(epsg=27700)\n",
    "\n",
    "# Load and clip depth-threshold shapefiles\n",
    "threshold_layers = {}\n",
    "for shp_path in glob.glob(\"data/flood_risk/*/*.shp\"):\n",
    "    gdf = gpd.read_file(shp_path)\n",
    "    if gdf.crs != grid_geom.crs:\n",
    "        gdf = gdf.to_crs(grid_geom.crs)\n",
    "    gdf = gdf.clip(grid_geom)\n",
    "\n",
    "    if not gdf.empty:\n",
    "        key = os.path.basename(shp_path)\n",
    "        threshold_layers[key] = gdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0481818d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEPTH_THRESHOLDS = [0.2, 0.3, 0.6, 0.9, 1.2]\n",
    "\n",
    "RISK_BAND_WEIGHTS = {\n",
    "    \"Very Low\": 0.0005, # (0 + 0.1)/2 \n",
    "    \"Low\": 0.00055, # (0.1 + 1)/2\n",
    "    \"Medium\": 0.0215, # (1 + 3.3)/2\n",
    "    \"High\": 0.033 # (3.3 + 3.3)/2\n",
    "}\n",
    "\n",
    "FILE_DEPTH = {\n",
    "    \"merged_rofsw_4bandPolygon.shp\": 0.0,\n",
    "    \"merged_rofsw_4band_0_2m_depthPolygon.shp\": 0.2,\n",
    "    \"merged_rofsw_4band_0_3m_depthPolygon.shp\": 0.3,\n",
    "    \"merged_rofsw_4band_0_6m_depthPolygon.shp\": 0.6,\n",
    "    \"merged_rofsw_4band_0_9m_depthPolygon.shp\": 0.9,\n",
    "    \"merged_rofsw_4band_1_2m_depthPolygon.shp\": 1.2\n",
    "}\n",
    "\n",
    "exceedance_probs = {}\n",
    "\n",
    "cell_area = grid_sample[\"geometry\"].area\n",
    "\n",
    "for file, layer in threshold_layers.items():\n",
    "    risk_weight = layer[\"risk_band\"].map(RISK_BAND_WEIGHTS)\n",
    "    weighted_area = layer.geometry.area * risk_weight\n",
    "    total_weighted_area = weighted_area.sum()\n",
    "    prob = total_weighted_area / cell_area\n",
    "    exceedance_probs[FILE_DEPTH[file]] = prob\n",
    "\n",
    "for depth in sorted(exceedance_probs):\n",
    "    prob = exceedance_probs[depth]\n",
    "    print(f\"Depth > {depth} m: Probability = {prob:.20f}\")\n",
    "\n",
    "print(exceedance_probs)\n",
    "\n",
    "breaks = [0, 0.2, 0.3, 0.6, 0.9, 1.2]\n",
    "probs = {(0.0, 0.0): 1 - exceedance_probs[0], (1.2, np.inf): exceedance_probs[1.2]}\n",
    "for i in range(len(breaks)-1):\n",
    "    a, b = breaks[i], breaks[i+1]\n",
    "    probs[(a, b)] = exceedance_probs[a] - exceedance_probs[b]\n",
    "print(probs)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae995939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample a range\n",
    "bins = list(probs.keys())\n",
    "range_probs = np.array(list(probs.values()))\n",
    "range_index = np.random.choice(len(bins), p=range_probs)\n",
    "low, high = bins[range_index]\n",
    "\n",
    "# Sample within the selected range\n",
    "if low == 0.0 and high == 0.0:\n",
    "    depth_sample = 0.0\n",
    "elif np.isinf(high):\n",
    "    depth_sample = low + np.random.exponential(scale=0.3)\n",
    "else:\n",
    "    depth_sample = np.random.uniform(low, high)\n",
    "\n",
    "print(\"\\nSelected depth range:\", (low, high))\n",
    "print(depth_sample)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4fcf58",
   "metadata": {},
   "source": [
    "### Depth-damage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118102ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/depth_damage.csv\")\n",
    "depths = np.array([float(c) for c in df.columns[1:]])\n",
    "\n",
    "# Compute overall damage fraction (mean across all types)\n",
    "overall_damage = df.iloc[:, 1:].astype(float).mean(axis=0).values\n",
    "\n",
    "# Define exponential damage function\n",
    "def exp_damage(d, k):\n",
    "    return 1 - np.exp(-k * d)\n",
    "\n",
    "# Fit the exponential model\n",
    "params, _ = curve_fit(exp_damage, depths, overall_damage, bounds=(0, np.inf))\n",
    "k = params[0]\n",
    "\n",
    "# Add noise and ensures stays within bounds\n",
    "damage_fraction_sample = np.clip(exp_damage(depth_sample, k) + np.random.normal(0, 0.05), 0, 1)\n",
    "print(damage_fraction_sample)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0363a0",
   "metadata": {},
   "source": [
    "## Other TEMP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a95a9cd",
   "metadata": {},
   "source": [
    "### Disability rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f779578",
   "metadata": {},
   "outputs": [],
   "source": [
    "disabled = pd.read_csv('data/disabled.csv')\n",
    "disabled_sample = sample_beta(disabled, 'Disability (3 categories)', 'Disabled under the Equality Act', 'Not disabled under the Equality Act', 'Observation')\n",
    "print(disabled_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a36ab1",
   "metadata": {},
   "source": [
    "### English proficiency rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db365f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_proficiency(category):\n",
    "    if category in [\n",
    "        \"Main language is English (English or Welsh in Wales)\",\n",
    "        \"Main language is not English (English or Welsh in Wales): Can speak English very well or well\"\n",
    "    ]:\n",
    "        return \"Good English Proficiency\"\n",
    "    elif category == \"Main language is not English (English or Welsh in Wales): Cannot speak English or cannot speak English well\":\n",
    "        return \"Bad English Proficiency\"\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "english = pd.read_csv(\"data/english_proficiency.csv\")   \n",
    "total_english = english.groupby('Proficiency in English language (4 categories)')['Observation'].sum()\n",
    "english['Proficiency_Group'] = english['Proficiency in English language (4 categories)'].apply(map_proficiency)\n",
    "\n",
    "english_sample = sample_beta(english, 'Proficiency_Group', 'Good English Proficiency', 'Bad English Proficiency', 'Observation')\n",
    "print(english_sample)\n",
    "english.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c43566",
   "metadata": {},
   "source": [
    "### General health rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fc474f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_health = pd.read_csv('data/general_health.csv')\n",
    "gen_health_sample = sample_beta(gen_health, 'General health (3 categories)', 'Good health', 'Not good health', 'Observation')\n",
    "print(gen_health_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb9f472",
   "metadata": {},
   "source": [
    "### Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22387550",
   "metadata": {},
   "outputs": [],
   "source": [
    "age = pd.read_csv('data/age.csv')\n",
    "age[\"children\"] = age[\"Age (6 categories)\"] == \"Aged 15 years and under\"\n",
    "age[\"elderly\"] = age[\"Age (6 categories)\"] == \"Aged 65 years and over\"\n",
    "age = (age.groupby([\"Lower tier local authorities\"])\n",
    "      .apply(lambda x: pd.Series({\n",
    "          \"total_children\": x.loc[x[\"children\"], \"Observation\"].sum(),\n",
    "          \"total_elderly\": x.loc[x[\"elderly\"], \"Observation\"].sum(),\n",
    "          \"total_not_elderly\": x.loc[~x[\"elderly\"], \"Observation\"].sum(),\n",
    "          \"total_not_children\": x.loc[~x[\"children\"], \"Observation\"].sum()\n",
    "      }))\n",
    "      .reset_index())\n",
    "\n",
    "age = age.melt(\n",
    "    id_vars=[\"Lower tier local authorities Code\", \"Lower tier local authorities\"],\n",
    "    var_name=\"category\",\n",
    "    value_name=\"Observation\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30e2b24",
   "metadata": {},
   "source": [
    "### Elderly rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124dfbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "elderly_sample = sample_beta(age, \"category\", \"total_elderly\", \"total_not_elderly\", \"Observation\")\n",
    "print(elderly_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f070dc",
   "metadata": {},
   "source": [
    "### Children rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57649f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "child_sample = sample_beta(age, \"category\", \"total_children\", \"total_not_children\", \"Observation\")\n",
    "print(child_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd1facc",
   "metadata": {},
   "source": [
    "### Vehicle rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977ee25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicle = pd.read_csv('data/vehicle.csv')\n",
    "vehicle_sample = sample_beta(vehicle, \"Car or van availability (3 categories)\", \"1 or more cars or vans in household\", \"No cars or vans in household\", \"Observation\")\n",
    "print(vehicle_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac8b150",
   "metadata": {},
   "source": [
    "### Second address rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0464c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "second_add = pd.read_csv('data/second_address.csv')\n",
    "second_add[\"second_address_combined\"] = second_add[\"Second address indicator (3 categories)\"].apply(\n",
    "    lambda x: \"No second address\" if x == \"No second address\" else \"Has second address\"\n",
    ")\n",
    "\n",
    "second_add_sample = sample_beta(second_add, \"second_address_combined\", \"Has second address\", \"No second address\", \"Observation\")\n",
    "print(second_add_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f1f846",
   "metadata": {},
   "source": [
    "## Derived"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c7d539",
   "metadata": {},
   "source": [
    "### Physical vulnerability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684f3dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    \"Mean building age\",\n",
    "    \"Impervious surface area\",\n",
    "    \"Distance to main road\",\n",
    "    \"Road network density\",\n",
    "    \"Distance to water\",\n",
    "    \"Water density\",\n",
    "    \"Elevation\",\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b00dbc",
   "metadata": {},
   "source": [
    "### Socioeconomic vulnerability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4241fd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    \"Elderly rate\",\n",
    "    \"Children rate\",\n",
    "    \"Disability rate\",\n",
    "    \"English proficiency\",\n",
    "    \"Low-income fraction\", ???\n",
    "    \"General health rate\",\n",
    "    household\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2165cb",
   "metadata": {},
   "source": [
    "### Preparedness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686907ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    \"Warning issued\",\n",
    "    \"Emergency response time\",\n",
    "    \"Ambulance handover delays\",\n",
    "    \"Hospital bed availability\",\n",
    "    \"Vehicle rate\",\n",
    "    Distance to hospital\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb2e88f",
   "metadata": {},
   "source": [
    "### Recovery capacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f153dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    \"Second address rate\",\n",
    "Mean property value\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e36638",
   "metadata": {},
   "source": [
    "### Exposure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6485e903",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Population density\n",
    "Holiday binary\n",
    "Probability of flood (risk)\n",
    "    Historic\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4621b948",
   "metadata": {},
   "source": [
    "### Overall vulnerability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe6fd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Physical vulnerability\n",
    "Socioeconomic vulnerability\n",
    "Preparedness\n",
    "Response capacity\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047a2bb4",
   "metadata": {},
   "source": [
    "### Impact score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08720b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Physical vulnerability\n",
    "Socioeconomic vulnerability\n",
    "Preparedness \n",
    "Response capacity\n",
    "Overall vulnerability\n",
    "Exposure\n",
    "Depth-damage curve\n",
    "Flood depth\n",
    "Population density\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
