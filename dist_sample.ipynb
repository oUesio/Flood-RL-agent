{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11896ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from scipy.stats import gamma, lognorm, norm, skewnorm\n",
    "import random\n",
    "import os\n",
    "import rasterio\n",
    "import glob\n",
    "import datetime as dt\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "def sample_categorical_census(df: pd.DataFrame, category_col: str, value_col: str, ignore_categories: list):\n",
    "    \"\"\"\n",
    "    Sample from a categorical distribution.\n",
    "    \"\"\"\n",
    "\n",
    "    # Filter ignored categories\n",
    "    if ignore_categories:\n",
    "        df = df[~df[category_col].isin(ignore_categories)]\n",
    "\n",
    "    totals = df.groupby(category_col)[value_col].sum()\n",
    "    probs = totals / totals.sum()\n",
    "\n",
    "    samples = np.random.choice(\n",
    "        probs.index.to_numpy(),\n",
    "        size=1,\n",
    "        p=probs.to_numpy()\n",
    "    )\n",
    "\n",
    "    return samples[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a6d969",
   "metadata": {},
   "source": [
    "## Hydrological"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb626db2",
   "metadata": {},
   "source": [
    "### 24-Hour Precipitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67abdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "precipitation = pd.read_csv(\"data/precipitation.csv\")\n",
    "rainfall = precipitation[\"rainfall\"]\n",
    "pct_zero = (rainfall == 0).sum() / len(rainfall)\n",
    "zero_sample = np.random.binomial(n=1, p=pct_zero, size=1)[0]\n",
    "\n",
    "precipitation_nonzero = precipitation[precipitation[\"rainfall\"] != 0].copy()\n",
    "rainfall_nonzero = precipitation[\"rainfall\"]\n",
    "max_prec = max(rainfall_nonzero)\n",
    "shape, loc, scale = gamma.fit(rainfall_nonzero)\n",
    "prec_sample = 0 if zero_sample == 1 else gamma.rvs(a=shape, loc=loc, scale=scale, size=1)[0]\n",
    "print(prec_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fd2f4a",
   "metadata": {},
   "source": [
    "### Groundwater level (WORK IN PREC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ef57c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose random groundwater station, Gamma\n",
    "\n",
    "# mAOD\n",
    "# add prec dependency, modify ground water level based on previous prec_sample\n",
    "\n",
    "groundwater_files = [f for f in os.listdir(\"data/groundwater_level\") if f.lower().endswith(\".csv\")]\n",
    "gw_file = random.choice(groundwater_files)\n",
    "\n",
    "path = os.path.join(\"data/groundwater_level\", gw_file)\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "values = pd.to_numeric(df[\"value\"], errors=\"coerce\").dropna()\n",
    "a, loc, scale = skewnorm.fit(values)\n",
    "gw_sample = skewnorm.rvs(a, loc, scale, size=1)[0]\n",
    "\n",
    "print(f\"Selected file: {gw_file}\")\n",
    "print(gw_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba97ce24",
   "metadata": {},
   "source": [
    "### River flow (WORK IN PREC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a3a8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random choose river station, Gamma\n",
    "\n",
    "# m3/s\n",
    "# add prec dependency, modify ground water level based on previous prec_sample\n",
    "\n",
    "river_flow_files = [f for f in os.listdir(\"data/river_flow\") if f.lower().endswith(\".csv\")]\n",
    "rf_file = random.choice(river_flow_files)\n",
    "\n",
    "path = os.path.join(\"data/river_flow\", rf_file)\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "values = pd.to_numeric(df[\"value\"], errors=\"coerce\").dropna()\n",
    "shape, loc, scale = gamma.fit(values, floc=0)\n",
    "rf_sample = gamma.rvs(a=shape, loc=loc, scale=scale, size=1)[0]\n",
    "\n",
    "print(f\"Selected file: {rf_file}\")\n",
    "print(rf_sample)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f12bb7",
   "metadata": {},
   "source": [
    "### River level (WORK IN PREC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8697fe6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using same river station as river flow, Gamma\n",
    "\n",
    "# m\n",
    "# add prec dependency, modify ground water level based on previous prec_sample\n",
    "\n",
    "file_prefix = rf_file.split('-')[0]\n",
    "river_level_files = [f for f in os.listdir(\"data/river_level\") if f.lower().endswith(\".csv\")]\n",
    "for f in river_level_files:\n",
    "    if file_prefix in f:\n",
    "        rl_file = f\n",
    "        break\n",
    "    \n",
    "path = os.path.join(\"data/river_level\", rl_file)\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "values = pd.to_numeric(df[\"value\"], errors=\"coerce\").dropna()\n",
    "shape, loc, scale = gamma.fit(values, floc=0)\n",
    "rl_sample = gamma.rvs(a=shape, loc=loc, scale=scale, size=1)[0]\n",
    "\n",
    "print(f\"Selected file: {rl_file}\")\n",
    "print(rl_sample)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e530fa6",
   "metadata": {},
   "source": [
    "## Other"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb0199d",
   "metadata": {},
   "source": [
    "### Urban/Rural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec72cb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "urban = pd.read_csv(\"data/urban_rural.csv\")\n",
    "counts = urban['Urban_rural_flag'].value_counts()\n",
    "urban_probs = counts / counts.sum()\n",
    "p_urban = urban_probs['Urban']\n",
    "urban_sample = np.random.binomial(n=1, p=p_urban, size=1)[0]\n",
    "print(urban_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132cb0df",
   "metadata": {},
   "source": [
    "### Population density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6786f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "popden = pd.read_csv(\"data/population_density.csv\", dtype={\"LAD2021\": \"string\", \"OA21CD\": \"string\", \"Total\": \"Int64\"})\n",
    "merged = popden.merge(urban, on=\"OA21CD\", how=\"left\")\n",
    "popden_urbanrural = merged[['LAD2021','OA21CD','Total','Urban_rural_flag']]\n",
    "\n",
    "flag = 'urban' if urban_sample == 1 else 'rural'\n",
    "popden_total  = popden_urbanrural[popden_urbanrural[\"Urban_rural_flag\"].str.lower() == flag][\"Total\"]\n",
    "\n",
    "log = np.log(popden_total)\n",
    "\n",
    "mu, sigma = log.mean(), log.std()\n",
    "\n",
    "popden_sample = np.random.lognormal(mean=mu, sigma=sigma, size=1)[0]\n",
    "print(popden_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8b737c",
   "metadata": {},
   "source": [
    "### Mean property value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4faa335f",
   "metadata": {},
   "outputs": [],
   "source": [
    "property = pd.read_csv(\"data/property_value.csv\")\n",
    "property = property.dropna(subset=['price', 'property_type', 'duration'])\n",
    "property = property[property['price'] > 0]\n",
    "\n",
    "log_property = np.log(property['price'])\n",
    "shape, loc, scale = skewnorm.fit(log_property)\n",
    "sample_log = skewnorm.rvs(shape, loc=loc, scale=scale, size=1)\n",
    "property_sample = np.exp(sample_log)[0]\n",
    "print(property_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a622c9",
   "metadata": {},
   "source": [
    "### Building age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb75b959",
   "metadata": {},
   "outputs": [],
   "source": [
    "building_age = pd.read_csv(\"data/property_age.csv\")\n",
    "age_columns = [\n",
    "    \"BP_PRE_1900\",\"BP_1900_1918\",\"BP_1919_1929\",\"BP_1930_1939\",\n",
    "    \"BP_1945_1954\",\"BP_1955_1964\",\"BP_1965_1972\",\"BP_1973_1982\",\n",
    "    \"BP_1983_1992\",\"BP_1993_1999\",\"BP_2000_2009\",\"BP_2010_2015\"\n",
    "]\n",
    "age_totals = building_age[age_columns].sum()\n",
    "age_totals.index = [\n",
    "    \"Pre-1900\",\"1900-1918\",\"1919-1929\",\"1930-1939\",\"1945-1954\",\"1955-1964\",\n",
    "    \"1965-1972\",\"1973-1982\",\"1983-1992\",\"1993-1999\",\"2000-2009\",\"2010-2015\"\n",
    "]\n",
    "age_probs = age_totals / age_totals.sum()\n",
    "age_categories = age_totals.index.tolist()\n",
    "building_age_sample = np.random.choice(age_categories, size=1, p=age_probs)[0]\n",
    "print(building_age_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace060ae",
   "metadata": {},
   "source": [
    "### Season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febedd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "season = ['Winter', 'Spring', 'Summer', 'Autumn']\n",
    "season_sample = random.choice(season)\n",
    "print(season_sample)\n",
    "\n",
    "SEASON_MONTHS = {\n",
    "    \"Spring\": [3, 4, 5],\n",
    "    \"Summer\": [6, 7, 8],\n",
    "    \"Autumn\": [9, 10, 11],\n",
    "    \"Winter\": [12, 1, 2],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7abfa9",
   "metadata": {},
   "source": [
    "### Holiday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c8d2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Min 28 days off annually\n",
    "'''\n",
    "p = 28 / 365\n",
    "holiday_binary_sample = 1 if random.random() < p else 0\n",
    "print(holiday_binary_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f8daca",
   "metadata": {},
   "source": [
    "### Emergency Response Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd104f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = pd.read_csv(\"data/response_times.csv\")\n",
    "def hhmmss_to_hours(s):\n",
    "    h, m, sec = map(int, s.split(\":\"))\n",
    "    return h + m/60 + sec/3600\n",
    "\n",
    "SEASON_MONTHS_FULL = {\n",
    "    \"Spring\": [\"March\", \"April\", \"May\"],\n",
    "    \"Summer\": [\"June\", \"July\", \"August\"],\n",
    "    \"Autumn\": [\"September\", \"October\", \"November\"],\n",
    "    \"Winter\": [\"December\", \"January\", \"February\"],\n",
    "}\n",
    "months = SEASON_MONTHS_FULL[season_sample]\n",
    "response_season = response[response['month'].isin(months)]\n",
    "\n",
    "c2_mean = response_season[\"C2_mean\"].apply(hhmmss_to_hours)\n",
    "c3_mean = response_season[\"C3_mean\"].apply(hhmmss_to_hours)\n",
    "c2_count = response_season['C2_count'].str.replace(',', '').astype(int).sum()\n",
    "c3_count = response_season['C3_count'].str.replace(',', '').astype(int).sum()\n",
    "c2_prob = c2_count / (c2_count + c3_count)\n",
    "c3_prob = c3_count / (c2_count + c3_count)\n",
    "response_category = np.random.choice(['C2', 'C3'], size=1, p=[c2_prob, c3_prob])[0]\n",
    "if response_category == 'C2':\n",
    "    shape, loc, scale = lognorm.fit(c2_mean, floc=0)\n",
    "else:\n",
    "    shape, loc, scale = lognorm.fit(c3_mean, floc=0)\n",
    "response_sample = lognorm.rvs(shape, loc=loc, scale=scale, size=1)[0]\n",
    "\n",
    "popden_mean = popden_total.mean()\n",
    "if popden_sample < popden_mean: # scale based on average population density\n",
    "    popden_factor = 1 - 0.5 * ((popden_mean - popden_sample) / popden_mean)  # reduce scale\n",
    "else:\n",
    "    popden_factor = 1 + 0.5 * ((popden_sample - popden_mean) / popden_mean)\n",
    "prec_factor = np.exp(prec_sample / 50) # small rain = minimal effect, heavy rain = large effect\n",
    "print(popden_factor, prec_factor)\n",
    "adjusted_response = response_sample * popden_factor * prec_factor\n",
    "print(adjusted_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec6153d",
   "metadata": {},
   "source": [
    "### Ambulance handover delays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a446029",
   "metadata": {},
   "outputs": [],
   "source": [
    "handover = pd.read_csv(\"data/ambulance_handover.csv\")\n",
    "\n",
    "for col in [\"Handover time known\", \"Over 15 minutes\", \"Over 30 minutes\", \"Over 60 minutes\", \"Handover time unknown\", \"All handovers\"]:\n",
    "    handover[col] = handover[col].str.replace(\",\", \"\").astype(int)\n",
    "\n",
    "handover[\"Under 15 min\"] = handover[\"Handover time known\"] - handover[\"Over 15 minutes\"]\n",
    "handover[\"15–30 min\"] = handover[\"Over 15 minutes\"] - handover[\"Over 30 minutes\"]\n",
    "handover[\"30–60 min\"] = handover[\"Over 30 minutes\"] - handover[\"Over 60 minutes\"]\n",
    "handover[\"Over 60 min\"] = handover[\"Over 60 minutes\"]\n",
    "\n",
    "handover[\"Date_parsed\"] = pd.to_datetime(handover[\"Date\"], format=\"%b'%y\")\n",
    "handover_season = handover[handover[\"Date_parsed\"].dt.month.isin(SEASON_MONTHS[season_sample])]\n",
    "\n",
    "counts = handover_season[[\"Under 15 min\", \"15–30 min\", \"30–60 min\", \"Over 60 min\"]].sum()\n",
    "probs = counts / counts.sum()\n",
    "\n",
    "samples = np.random.choice(probs.index.to_numpy(),size=1,p=probs.to_numpy())\n",
    "handover_sample = samples[0]\n",
    "print(handover_sample)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383c6272",
   "metadata": {},
   "source": [
    "### Hospital bed availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4959703e",
   "metadata": {},
   "outputs": [],
   "source": [
    "beds = pd.read_csv(\"data/hospital_beds.csv\")\n",
    "\n",
    "beds[\"Period\"] = pd.to_datetime(beds[\"Period\"], format=\"%d/%m/%Y\")\n",
    "# Aggregate by month (sum across all hospitals)\n",
    "beds = beds.groupby('Period')[['Available', 'Occupied', 'Free']].sum().reset_index()\n",
    "\n",
    "# Compute occupancy percentage\n",
    "beds['OccupancyPct'] = beds['Occupied'] / beds['Available']\n",
    "\n",
    "beds_season = beds[beds[\"Period\"].dt.month.isin(SEASON_MONTHS[season_sample])]\n",
    "values = beds_season[\"OccupancyPct\"].dropna().values\n",
    "\n",
    "mu, sigma = norm.fit(values)\n",
    "occ_pct_sample = np.clip(norm.rvs(mu, sigma), 0, 1)\n",
    "\n",
    "print(occ_pct_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51942825",
   "metadata": {},
   "source": [
    "## Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4626e5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates overall grid shapefile for watercourse, flood risk, elevation, impervious surface area, historic flood, road, hospital locations\n",
    "# recrop area using boundary shapefile for handling updated shapefile/tiff files (GM_shapefile/CAUTH_MAY_2025_EN_BSC.shp)\n",
    "# ignore cell if on or past boundary\n",
    "# cells of size 1km x 1km\n",
    "# centroids used for distance calculations\n",
    "''' \n",
    "data/watercourse/Watercourse.shp\n",
    "- closest watercourse to centre of cell (m)\n",
    "- density of watercourses in cell\n",
    "data/flood_risk/rofsw_4bandPolygon/merged_rofsw_4bandPolygon.shp\n",
    "- confidence-weighted average risk\n",
    "data/elevation.tif\n",
    "- average elevation in cell (m)\n",
    "data/impervious_surface.tif\n",
    "- fraction of impervious surface in cell\n",
    "data/historic_flood_map/Historic_Flood_MapPolygon.shp\n",
    "- if cell has been flooded in the past\n",
    "data/road/RoadLink.shp\n",
    "- closest major road to centre of cell (km)\n",
    "- density of roads in cell\n",
    "data/hospital_locations/hospital_locations.shp\n",
    "- distance to nearest hospital (km)\n",
    "'''\n",
    "\n",
    "'''CELL_SIZE = 1000  # metres\n",
    "CELL_AREA_M2 = CELL_SIZE ** 2\n",
    "CELL_AREA_KM2 = CELL_AREA_M2 / 1e6\n",
    "\n",
    "WATERCOURSE = \"data/watercourse/Watercourse.shp\"\n",
    "FLOOD_RISK = \"data/flood_risk/rofsw_4bandPolygon/merged_rofsw_4bandPolygon.shp\"\n",
    "ELEVATION = \"data/elevation.tif\"\n",
    "IMPERVIOUS = \"data/impervious.tif\"\n",
    "HISTORIC_FLOOD = \"data/historic_flood_map/Historic_Flood_MapPolygon.shp\"\n",
    "ROAD = \"data/road/RoadLink.shp\"\n",
    "HOSPITAL = \"data/hospital_locations/hospital_locations.shp\"\n",
    "BOUNDARY = \"GM_shapefile/CAUTH_MAY_2025_EN_BSC.shp\"\n",
    "\n",
    "OUTPUT = \"data/grid/grid.shp\"\n",
    "\n",
    "RISK_SCORES = {\"Very low\": 1, \"Low\": 2, \"Medium\": 3, \"High\": 4}\n",
    "RISK_SCORES = {\n",
    "    \"Very Low\": 0.0005, # (0 + 0.1)/2 \n",
    "    \"Low\": 0.00055, # (0.1 + 1)/2\n",
    "    \"Medium\": 0.0215, # (1 + 3.3)/2\n",
    "    \"High\": 0.033 # (3.3 + 3.3)/2\n",
    "}\n",
    "\n",
    "def build_grid(boundary):\n",
    "    minx, miny, maxx, maxy = boundary.total_bounds\n",
    "\n",
    "    xs = np.arange(minx, maxx, CELL_SIZE)\n",
    "    ys = np.arange(miny, maxy, CELL_SIZE)\n",
    "\n",
    "    grid = gpd.GeoDataFrame(\n",
    "        geometry=[box(x, y, x + CELL_SIZE, y + CELL_SIZE) for x in xs for y in ys],\n",
    "        crs=\"EPSG:27700\"\n",
    "    )\n",
    "\n",
    "    grid = grid[grid.geometry.within(boundary.geometry.union_all())]\n",
    "    grid[\"cell_id\"] = grid.index\n",
    "    return grid\n",
    "\n",
    "def line_density(grid, lines, colname, cell_size):\n",
    "    grid[colname] = 0.0\n",
    "    cell_area_km2 = (cell_size * cell_size) / 1e6  # km²\n",
    "    for i, cell in grid.geometry.items():\n",
    "        inter = lines.geometry.intersection(cell)\n",
    "        length_m = sum(geom.length for geom in inter if not geom.is_empty)\n",
    "        grid.at[i, colname] = (length_m / 1000) / cell_area_km2\n",
    "    return grid\n",
    "\n",
    "\n",
    "def nearest_distance(grid, targets, colname, km=True):\n",
    "    centroids = grid.copy()\n",
    "    centroids[\"geometry\"] = centroids.geometry.centroid\n",
    "\n",
    "    nearest = gpd.sjoin_nearest(\n",
    "        centroids,\n",
    "        targets[[\"geometry\"]],\n",
    "        how=\"left\",\n",
    "        distance_col=colname\n",
    "    )\n",
    "\n",
    "    dist = nearest.groupby(nearest.index)[colname].first()\n",
    "    if km:\n",
    "        dist = dist / 1000\n",
    "\n",
    "    grid[colname] = dist\n",
    "    return grid\n",
    "\n",
    "\n",
    "def flood_risk_score(grid, risk):\n",
    "    inter = gpd.overlay(\n",
    "        grid[[\"cell_id\", \"geometry\"]],\n",
    "        risk,\n",
    "        how=\"intersection\"\n",
    "    )\n",
    "\n",
    "    if inter.empty:\n",
    "        grid[\"risk_score\"] = 0.0\n",
    "        return grid\n",
    "\n",
    "    inter[\"area\"] = inter.geometry.area\n",
    "    inter[\"risk_value\"] = inter[\"risk_band\"].map(RISK_SCORES).fillna(0)\n",
    "    inter[\"conf_weight\"] = inter[\"confidence\"] / 10\n",
    "\n",
    "    inter[\"num\"] = inter[\"area\"] * inter[\"risk_value\"] * inter[\"conf_weight\"]\n",
    "    inter[\"den\"] = inter[\"area\"] * inter[\"conf_weight\"]\n",
    "\n",
    "    agg = inter.groupby(\"cell_id\")[[\"num\", \"den\"]].sum()\n",
    "    grid[\"risk_score\"] = (agg[\"num\"] / agg[\"den\"]).reindex(grid.cell_id).fillna(0)\n",
    "    return grid\n",
    "\n",
    "\n",
    "def zonal_mean(grid, raster_path, colname):\n",
    "    stats = zonal_stats(\n",
    "        grid.geometry,\n",
    "        raster_path,\n",
    "        stats=\"mean\",\n",
    "        nodata=0\n",
    "    )\n",
    "    grid[colname] = [s[\"mean\"] for s in stats]\n",
    "    return grid\n",
    "\n",
    "\n",
    "def zonal_fraction_nonzero(grid, raster_path, colname):\n",
    "    stats = zonal_stats(\n",
    "        grid.geometry,\n",
    "        raster_path,\n",
    "        stats=[\"count\", \"nodata\"],\n",
    "        add_stats={\"nonzero\": lambda x: np.count_nonzero(x)}\n",
    "    )\n",
    "    grid[colname] = [\n",
    "        s[\"nonzero\"] / s[\"count\"] if s[\"count\"] else 0\n",
    "        for s in stats\n",
    "    ]\n",
    "    return grid\n",
    "\n",
    "\n",
    "def historic_flood_flag(grid, historic):\n",
    "    inter = gpd.overlay(\n",
    "        grid[[\"cell_id\", \"geometry\"]],\n",
    "        historic,\n",
    "        how=\"intersection\"\n",
    "    )\n",
    "    inter[\"area\"] = inter.geometry.area\n",
    "    flooded = inter.groupby(\"cell_id\")[\"area\"].sum()\n",
    "\n",
    "    grid[\"historic\"] = (\n",
    "        flooded / CELL_AREA_M2 > 0.5\n",
    "    ).reindex(grid.cell_id).fillna(False).astype(int)\n",
    "\n",
    "    return grid\n",
    "\n",
    "\n",
    "# LOAD DATA\n",
    "print(\"Loading data...\")\n",
    "\n",
    "water = gpd.read_file(WATERCOURSE).set_crs(epsg=27700, allow_override=True)\n",
    "risk = gpd.read_file(FLOOD_RISK).set_crs(epsg=27700, allow_override=True)\n",
    "historic = gpd.read_file(HISTORIC_FLOOD).set_crs(epsg=27700, allow_override=True)\n",
    "road = gpd.read_file(ROAD).set_crs(epsg=27700, allow_override=True)\n",
    "hospital = gpd.read_file(HOSPITAL).set_crs(epsg=27700, allow_override=True)\n",
    "boundary = gpd.read_file(BOUNDARY).set_crs(epsg=27700, allow_override=True)\n",
    "\n",
    "# GRID\n",
    "print(\"Building grid...\")\n",
    "grid = build_grid(boundary)\n",
    "grid.to_file(\"data/grid/grid_step_01.shp\")\n",
    "\n",
    "# LINE DENSITIES\n",
    "print(\"Calculating watercourse density...\")\n",
    "grid = line_density(grid, water, \"water_dens\", CELL_SIZE)\n",
    "\n",
    "print(\"Calculating road density...\")\n",
    "grid = line_density(grid, road, \"road_dens\", CELL_SIZE)\n",
    "\n",
    "# NEAREST DISTANCES\n",
    "print(\"Calculating nearest watercourse distance...\")\n",
    "grid = nearest_distance(grid, water, \"water_dist\", km=False)\n",
    "\n",
    "print(\"Calculating nearest hospital distance...\")\n",
    "grid = nearest_distance(grid, hospital, \"hospital\")\n",
    "\n",
    "print(\"Calculating nearest major road distance...\")\n",
    "major_roads = road[road[\"function\"].isin([\"A Road\", \"Motorway\"])]\n",
    "grid = nearest_distance(grid, major_roads, \"road_dist\")\n",
    "\n",
    "# FLOOD RISK\n",
    "print(\"Calculating flood risk score...\")\n",
    "grid = flood_risk_score(grid, risk)\n",
    "\n",
    "# RASTER FEATURES\n",
    "print(\"Calculating elevation...\")\n",
    "grid = zonal_mean(grid, ELEVATION, \"elevation\")\n",
    "\n",
    "print(\"Calculating impervious fraction...\")\n",
    "grid = zonal_fraction_nonzero(grid, IMPERVIOUS, \"impervious\")\n",
    "\n",
    "# HISTORIC FLOOD\n",
    "print(\"Calculating historic flood flag...\")\n",
    "grid = historic_flood_flag(grid, historic)\n",
    "\n",
    "# SAVE OUTPUT\n",
    "cols = [\n",
    "    \"water_dens\", \"water_dist\", \"risk_score\", \"elevation\",\n",
    "    \"impervious\", \"historic\", \"road_dens\", \"road_dist\", \"hospital\", \"geometry\"\n",
    "]\n",
    "\n",
    "grid[cols].to_file(OUTPUT)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8da0865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from overall grid shapefile\n",
    "gdf = gpd.read_file('data/grid/grid.shp')\n",
    "features = [\n",
    "    \"water_dens\", \"water_dist\", \"risk_score\", \"elevation\",\n",
    "    \"impervious\", \"historic\", \"road_dens\", \"road_dist\", \"hospital\"\n",
    "]\n",
    "\n",
    "def sample_cell_with_noise(gdf, features, noise_scale=0.05):\n",
    "    row = gdf.sample(1).iloc[0]\n",
    "    sample = {}\n",
    "    for f in features:\n",
    "        val = row[f]\n",
    "        noise = np.random.normal(0, noise_scale * abs(val + 1e-6))\n",
    "        sample[f] = max(val + noise, 0)\n",
    "    sample[\"geometry\"] = row.geometry \n",
    "    return sample\n",
    "\n",
    "def sample_neighborhood(gdf, features, k=5):\n",
    "    cell = gdf.sample(1)\n",
    "    dists = gdf.geometry.distance(cell.geometry.iloc[0])\n",
    "    neighbors = gdf.loc[dists.nsmallest(k).index]\n",
    "    return neighbors[features].mean().to_dict()\n",
    "\n",
    "grid_sample = sample_cell_with_noise(gdf, features)\n",
    "neighborhood_sample = sample_neighborhood(gdf, features)\n",
    "print(grid_sample)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1567f536",
   "metadata": {},
   "source": [
    "### Soil moisture saturation (WORK IN PREC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868df05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose pixel that contains the overall grid cell chosen\n",
    "# sample from normal distribution of time-series values for the pixel\n",
    "# filters by season\n",
    "\n",
    "cell_geom = grid_sample[\"geometry\"]  # polygon of the sampled grid cell\n",
    "\n",
    "# Load soil moisture rasters\n",
    "tiff_files = sorted(glob.glob(\"data/soil_moisture/*.tif\"))\n",
    "season_tiffs = []\n",
    "\n",
    "for f in tiff_files:\n",
    "    # Extract date from filename: dt_smuk_2023-12-22.tif\n",
    "    date_str = f.split(\"_\")[-1].replace(\".tif\", \"\")\n",
    "    file_date = dt.datetime.strptime(date_str, \"%Y-%m-%d\")\n",
    "    if file_date.month in SEASON_MONTHS[season_sample]:\n",
    "        season_tiffs.append(f)\n",
    "\n",
    "# Load soil moisture rasters\n",
    "stack = []\n",
    "with rasterio.open(season_tiffs[0]) as src:\n",
    "    transform = src.transform\n",
    "    nodata = src.nodata\n",
    "    for f in season_tiffs:\n",
    "        with rasterio.open(f) as s:\n",
    "            data = s.read(1).astype(np.float32)\n",
    "            if nodata is not None:\n",
    "                data[data == nodata] = np.nan\n",
    "            stack.append(data)\n",
    "\n",
    "stack = np.stack(stack, axis=0)  # shape: (time, rows, cols)\n",
    "\n",
    "# Find the single pixel containing the centroid of the sampled grid cell\n",
    "centroid_x, centroid_y = cell_geom.centroid.x, cell_geom.centroid.y\n",
    "col, row = ~transform * (centroid_x, centroid_y)\n",
    "row, col = int(row), int(col)\n",
    "\n",
    "# Ensure row/col are within raster bounds\n",
    "row = np.clip(row, 0, stack.shape[1]-1)\n",
    "col = np.clip(col, 0, stack.shape[2]-1)\n",
    "\n",
    "# Extract time series for that pixel\n",
    "values = stack[:, row, col]\n",
    "values = values[~np.isnan(values)]\n",
    "\n",
    "# Fit normal distribution safely\n",
    "if len(values) == 0:\n",
    "    soil_sample = np.nan\n",
    "else:\n",
    "    mu, sigma = norm.fit(values)\n",
    "    sigma = max(sigma, 1e-6)\n",
    "    soil_sample = norm.rvs(mu, sigma)\n",
    "\n",
    "print(soil_sample)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96885526",
   "metadata": {},
   "source": [
    "### Flood depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c785ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "clipped_data = {}\n",
    "\n",
    "for shp_path in glob.glob('data/flood_risk/*/*.shp'):\n",
    "    gdf_shp = gpd.read_file(shp_path)\n",
    "    gdf_shp_clipped = gdf_shp.clip(grid_sample[\"geometry\"].bounds)\n",
    "    \n",
    "    # Store clipped data\n",
    "    key = os.path.basename(shp_path).replace(\".shp\",\"\")\n",
    "    clipped_data[key] = gdf_shp_clipped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa9a529",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_geom = gpd.GeoSeries([grid_sample[\"geometry\"]])\n",
    "if grid_geom.crs is None:\n",
    "    grid_geom = grid_geom.set_crs(epsg=27700)\n",
    "\n",
    "# Load and clip depth-threshold shapefiles\n",
    "threshold_layers = {}\n",
    "for shp_path in glob.glob(\"data/flood_risk/*/*.shp\"):\n",
    "    gdf = gpd.read_file(shp_path)\n",
    "    if gdf.crs != grid_geom.crs:\n",
    "        gdf = gdf.to_crs(grid_geom.crs)\n",
    "    gdf = gdf.clip(grid_geom)\n",
    "\n",
    "    if not gdf.empty:\n",
    "        key = os.path.basename(shp_path)\n",
    "        threshold_layers[key] = gdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0481818d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEPTH_THRESHOLDS = [0.2, 0.3, 0.6, 0.9, 1.2]\n",
    "\n",
    "RISK_BAND_WEIGHTS = {\n",
    "    \"Very Low\": 0.0005, # (0 + 0.1)/2 \n",
    "    \"Low\": 0.00055, # (0.1 + 1)/2\n",
    "    \"Medium\": 0.0215, # (1 + 3.3)/2\n",
    "    \"High\": 0.033 # (3.3 + 3.3)/2\n",
    "}\n",
    "\n",
    "FILE_DEPTH = {\n",
    "    \"merged_rofsw_4bandPolygon.shp\": 0.0,\n",
    "    \"merged_rofsw_4band_0_2m_depthPolygon.shp\": 0.2,\n",
    "    \"merged_rofsw_4band_0_3m_depthPolygon.shp\": 0.3,\n",
    "    \"merged_rofsw_4band_0_6m_depthPolygon.shp\": 0.6,\n",
    "    \"merged_rofsw_4band_0_9m_depthPolygon.shp\": 0.9,\n",
    "    \"merged_rofsw_4band_1_2m_depthPolygon.shp\": 1.2\n",
    "}\n",
    "\n",
    "exceedance_probs = {}\n",
    "\n",
    "cell_area = grid_sample[\"geometry\"].area\n",
    "\n",
    "for file, layer in threshold_layers.items():\n",
    "    risk_weight = layer[\"risk_band\"].map(RISK_BAND_WEIGHTS)\n",
    "    weighted_area = layer.geometry.area * risk_weight\n",
    "    total_weighted_area = weighted_area.sum()\n",
    "    prob = total_weighted_area / cell_area\n",
    "    exceedance_probs[FILE_DEPTH[file]] = prob\n",
    "\n",
    "for depth in sorted(exceedance_probs):\n",
    "    prob = exceedance_probs[depth]\n",
    "    print(f\"Depth > {depth} m: Probability = {prob:.20f}\")\n",
    "\n",
    "print(exceedance_probs)\n",
    "\n",
    "breaks = [0, 0.2, 0.3, 0.6, 0.9, 1.2]\n",
    "probs = {(0.0, 0.0): 1 - exceedance_probs[0], (1.2, np.inf): exceedance_probs[1.2]}\n",
    "for i in range(len(breaks)-1):\n",
    "    a, b = breaks[i], breaks[i+1]\n",
    "    probs[(a, b)] = exceedance_probs[a] - exceedance_probs[b]\n",
    "print(probs)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae995939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample a range\n",
    "bins = list(probs.keys())\n",
    "range_probs = np.array(list(probs.values()))\n",
    "range_index = np.random.choice(len(bins), p=range_probs)\n",
    "low, high = bins[range_index]\n",
    "\n",
    "# Sample within the selected range\n",
    "if low == 0.0 and high == 0.0:\n",
    "    depth_sample = 0.0\n",
    "elif np.isinf(high):\n",
    "    depth_sample = low + np.random.exponential(scale=0.3)\n",
    "else:\n",
    "    depth_sample = np.random.uniform(low, high)\n",
    "\n",
    "print(\"\\nSelected depth range:\", (low, high))\n",
    "print(depth_sample)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4fcf58",
   "metadata": {},
   "source": [
    "### Depth-damage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118102ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/depth_damage.csv\")\n",
    "depths = np.array([float(c) for c in df.columns[1:]])\n",
    "\n",
    "# Compute overall damage fraction (mean across all types)\n",
    "overall_damage = df.iloc[:, 1:].astype(float).mean(axis=0).values\n",
    "\n",
    "# Define exponential damage function\n",
    "def exp_damage(d, k):\n",
    "    return 1 - np.exp(-k * d)\n",
    "\n",
    "# Fit the exponential model\n",
    "params, _ = curve_fit(exp_damage, depths, overall_damage, bounds=(0, np.inf))\n",
    "k = params[0]\n",
    "\n",
    "# Add noise and ensures stays within bounds\n",
    "damage_fraction_sample = np.clip(exp_damage(depth_sample, k) + np.random.normal(0, 0.05), 0, 1)\n",
    "print(damage_fraction_sample)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0363a0",
   "metadata": {},
   "source": [
    "## Other TEMP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a95a9cd",
   "metadata": {},
   "source": [
    "### Disability rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f779578",
   "metadata": {},
   "outputs": [],
   "source": [
    "disability = pd.read_csv(\"data/disabled.csv\")\n",
    "total_disability = disability.groupby('Disability (3 categories)')['Observation'].sum()\n",
    "\n",
    "alpha = total_disability['Disabled under the Equality Act']\n",
    "beta = total_disability['Not disabled under the Equality Act']\n",
    "\n",
    "disabled_sample = np.random.beta(alpha, beta, size=1)[0]\n",
    "print(disabled_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a36ab1",
   "metadata": {},
   "source": [
    "### English proficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db365f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_proficiency(category):\n",
    "    if category in [\n",
    "        \"Main language is English (English or Welsh in Wales)\",\n",
    "        \"Main language is not English (English or Welsh in Wales): Can speak English very well or well\"\n",
    "    ]:\n",
    "        return \"Good English Proficiency\"\n",
    "    elif category == \"Main language is not English (English or Welsh in Wales): Cannot speak English or cannot speak English well\":\n",
    "        return \"Bad English Proficiency\"\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "english = pd.read_csv(\"data/english_proficiency.csv\")   \n",
    "total_english = english.groupby('Proficiency in English language (4 categories)')['Observation'].sum()\n",
    "english['Proficiency_Group'] = english['Proficiency in English language (4 categories)'].apply(map_proficiency)\n",
    "grouped_english = english.groupby('Proficiency_Group')['Observation'].sum()\n",
    "#english_probs = grouped_english / grouped_english.sum()\n",
    "\n",
    "alpha = grouped_english['Good English Proficiency'] + 1\n",
    "beta = grouped_english['Bad English Proficiency'] + 1\n",
    "\n",
    "english_sample = np.random.beta(alpha, beta, size=1)[0]\n",
    "print(english_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c43566",
   "metadata": {},
   "source": [
    "### General health"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fc474f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/general_health.csv')\n",
    "gen_health_sample = sample_categorical_census(df, \n",
    "                                                'General health (4 categories)', \n",
    "                                                'Observation', \n",
    "                                                ['Does not apply'])\n",
    "print(gen_health_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30e2b24",
   "metadata": {},
   "source": [
    "### Elderly rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124dfbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "age = pd.read_csv('data/age.csv')\n",
    "elderly_sample = 1 \n",
    "print(elderly_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f070dc",
   "metadata": {},
   "source": [
    "### Children rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57649f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "child_sample = 1\n",
    "print(child_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd1facc",
   "metadata": {},
   "source": [
    "### Vehicle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977ee25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicle = pd.read_csv('data/vehicle.csv')\n",
    "vehicle_sample = 1\n",
    "print(vehicle_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac8b150",
   "metadata": {},
   "source": [
    "### Second address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0464c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "second_add = pd.read_csv('data/second_address.csv')\n",
    "second_add_sample = 1\n",
    "print(second_add_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f1f846",
   "metadata": {},
   "source": [
    "## Derived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb1cdc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "UNUSED:\n",
    "- disabled_sample -- CHANGE TO VARIABLE PERCENTAGE\n",
    "- english_sample\n",
    "- low_income_sample\n",
    "- property_sample\n",
    "- income_sample (used by low_income_sample)\n",
    "- building_age_sample -- TURN INTO ACTUAL NUMBER NOT RANGE\n",
    "- adjusted_response\n",
    "- depth_sample (used by damage_fraction_sample)\n",
    "- damage_fraction_sample\n",
    "- grid_sample\n",
    "    \"water_dens\", \"water_dist\", \"risk_score\", \"elevation\",\n",
    "    \"impervious\", \"historic\", \"road_dens\", \"road_dist\", \"hospital\"\n",
    "- gen_health_sample -- MAYBE TURN INTO PERCENTAGE SOMEHOW OR REMOVE ENTIRELY\n",
    "- handover_sample -- TURN INTO ACTUAL NUMBER IN MINS NOT RANGE\n",
    "- occ_pct_sample\n",
    "- elderly -- TURN INTO PERCENTAGE\n",
    "- children -- TURN INTO PERCENTAGE\n",
    "- vehicle_sample -- TURN INTO PERCENTAGE\n",
    "- second address -- TURN INTO PERCENTAGE\n",
    "\n",
    "UNUSED, use for depth:\n",
    "- soil_sample\n",
    "- gw_sample\n",
    "- rf_sample\n",
    "- rl_sample\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c7d539",
   "metadata": {},
   "source": [
    "### Physical vulnerability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684f3dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    \"Mean building age\",\n",
    "    \"Proportion of buildings brick/stone\",\n",
    "    \"Impervious surface area\",\n",
    "    \"Drainage\",\n",
    "    \"Road network density\",\n",
    "    \"Distance to water\",\n",
    "    \"Urban/rural\",\n",
    "    \"Elevation above sea level\",\n",
    "    \"Population density\",\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b00dbc",
   "metadata": {},
   "source": [
    "### Socioeconomic vulnerability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4241fd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    \"Age\",\n",
    "    \"Elderly rate\",\n",
    "    \"Children rate\",\n",
    "    \"Disability rate\",\n",
    "    \"English proficiency\",\n",
    "    \"Mean income\",\n",
    "    \"Low-income fraction\",\n",
    "    \"Employment history\",\n",
    "    \"Highest qualification\",\n",
    "    \"Economic activity status\",\n",
    "    \"Occupation current\",\n",
    "    \"Ns-Sec\",\n",
    "    \"Tenure of household\",\n",
    "    \"HBAI statistics\",\n",
    "    \"General health\",\n",
    "    \"Long-term health condition\",\n",
    "    \"Mean property value\",\n",
    "    \"Vehicle\",\n",
    "    \"Second address\",\n",
    "    \"Accomodation type\",\n",
    "    \"Household\",\n",
    "    \"Deprived in education\",\n",
    "    \"Deprived in employment\",\n",
    "    \"Deprived in health and disability\",\n",
    "    \"Deprived in housing\",\n",
    "    \"Household size\",\n",
    "    \"Families in household\",\n",
    "    \"Adults and children in household\",\n",
    "    \"Adults employed in household\",\n",
    "    \"Disabled in household\",\n",
    "    \"Long-term health in household\",\n",
    "    \"People per room in household\",\n",
    "    \"Occupancy rating for rooms\",\n",
    "    \"Household composition\",\n",
    "    \"Household deprivation\",\n",
    "    \"Lifestage of HRP\",\n",
    "    \"Mental health cost\",\n",
    "    \"Mental health types\",\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2165cb",
   "metadata": {},
   "source": [
    "### Preparedness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686907ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    \"Warning issued\",\n",
    "    \"Emergency response time\",\n",
    "    \"Access to communications\",\n",
    "    \"Household access to internet\",\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb2e88f",
   "metadata": {},
   "source": [
    "### Recovery capacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f153dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    \"Ambulance handover delays\",\n",
    "    \"Hospital bed availability\",\n",
    "    \"Hospital locations\",\n",
    "    \"Reconstruction time\",\n",
    "    \"Home insurance coverage\",\n",
    "    \"Health insurance coverage\",\n",
    "    \"Local government budget\",\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e36638",
   "metadata": {},
   "source": [
    "### Exposure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6485e903",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Population density\n",
    "Mean property value\n",
    "Holiday binary\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4621b948",
   "metadata": {},
   "source": [
    "### Overall vulnerability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe6fd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Physical vulnerability\n",
    "Socioeconomic vulnerability\n",
    "Preparedness\n",
    "Response capacity\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047a2bb4",
   "metadata": {},
   "source": [
    "### Impact score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08720b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Physical vulnerability\n",
    "Socioeconomic vulnerability\n",
    "Preparedness \n",
    "Response capacity\n",
    "Overall vulnerability\n",
    "Exposure\n",
    "Depth-damage curve\n",
    "Flood depth\n",
    "Population density\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
